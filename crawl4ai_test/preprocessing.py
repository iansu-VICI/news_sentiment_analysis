#!/usr/bin/env python3
"""
æ–‡æœ¬é è™•ç†å’Œå‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼
åŠŸèƒ½ï¼š
1. æ‰¹æ¬¡è™•ç†å¤šå€‹æ–‡ä»¶çš„ä¸»è¦å…§å®¹æå–
2. å»ºç«‹TF-IDFå’Œèªç¾©å‘é‡è³‡æ–™åº«
3. æ”¯æ´å¢é‡æ›´æ–°å’Œå¿«å–
4. æä¾›å¿«é€Ÿç›¸ä¼¼åº¦æŸ¥è©¢æ¥å£

ä½¿ç”¨æ–¹å¼ï¼š
python preprocessing.py --input-dir articles/ --output-dir vectors/ --method both
"""

import os
import re
import json
import pickle
import hashlib
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
import time

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

# å˜—è©¦å°å…¥sentence-transformersï¼ˆå¯é¸ï¼‰
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# ä¸‹è¼‰å¿…è¦çš„NLTKæ•¸æ“š
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("ä¸‹è¼‰stopwordsæ•¸æ“š...")
    nltk.download('stopwords')

class DocumentVectorizer:
    """æ–‡æª”å‘é‡åŒ–è™•ç†å™¨"""
    
    def __init__(self, cache_dir="vector_cache", use_semantic=True):
        """
        åˆå§‹åŒ–å‘é‡åŒ–è™•ç†å™¨
        
        Args:
            cache_dir: å¿«å–ç›®éŒ„
            use_semantic: æ˜¯å¦ä½¿ç”¨èªç¾©å‘é‡ï¼ˆéœ€è¦sentence-transformersï¼‰
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–NLPå·¥å…·
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)
        
        # åˆå§‹åŒ–å‘é‡åŒ–å™¨
        self.tfidf_vectorizer = None
        self.semantic_model = None
        self.use_semantic = use_semantic
        
        # å¿«å–
        self.document_cache = {}
        self.tfidf_cache = {}
        self.semantic_cache = {}
        
        # è¼‰å…¥èªç¾©æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if use_semantic and SENTENCE_TRANSFORMERS_AVAILABLE:
            print("ğŸ”„ è¼‰å…¥èªç¾©å‘é‡æ¨¡å‹...")
            start_time = time.time()
            self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
            load_time = time.time() - start_time
            print(f"âœ… èªç¾©å‘é‡æ¨¡å‹è¼‰å…¥å®Œæˆ ({load_time:.2f}ç§’)")
        elif use_semantic:
            print("âš ï¸  sentence-transformersæœªå®‰è£ï¼Œåªä½¿ç”¨TF-IDFæ–¹æ³•")
            print("   å®‰è£æ–¹æ³•: pip install sentence-transformers")
            self.use_semantic = False
        
        # è¼‰å…¥ç¾æœ‰å¿«å–
        self._load_cache()
    
    def _get_file_hash(self, file_path: str) -> str:
        """è¨ˆç®—æ–‡ä»¶çš„MD5é›œæ¹Šå€¼ç”¨æ–¼å¿«å–"""
        with open(file_path, 'rb') as f:
            content = f.read()
        return hashlib.md5(content).hexdigest()
    
    def _load_cache(self):
        """è¼‰å…¥ç¾æœ‰å¿«å–"""
        cache_files = {
            'documents': self.cache_dir / 'documents.json',
            'tfidf': self.cache_dir / 'tfidf_vectors.pkl',
            'semantic': self.cache_dir / 'semantic_vectors.pkl'
        }
        
        for cache_type, cache_file in cache_files.items():
            if cache_file.exists():
                try:
                    if cache_type == 'documents':
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            self.document_cache = json.load(f)
                    else:
                        with open(cache_file, 'rb') as f:
                            if cache_type == 'tfidf':
                                self.tfidf_cache = pickle.load(f)
                            else:
                                self.semantic_cache = pickle.load(f)
                    print(f"âœ… è¼‰å…¥ {cache_type} å¿«å–: {len(getattr(self, f'{cache_type}_cache'))} é …ç›®")
                except Exception as e:
                    print(f"âš ï¸  è¼‰å…¥ {cache_type} å¿«å–å¤±æ•—: {e}")
    
    def _save_cache(self):
        """ä¿å­˜å¿«å–"""
        cache_files = {
            'documents': (self.cache_dir / 'documents.json', self.document_cache),
            'tfidf': (self.cache_dir / 'tfidf_vectors.pkl', self.tfidf_cache),
            'semantic': (self.cache_dir / 'semantic_vectors.pkl', self.semantic_cache)
        }
        
        for cache_type, (cache_file, cache_data) in cache_files.items():
            if cache_data:  # åªæœ‰ç•¶å¿«å–æœ‰è³‡æ–™æ™‚æ‰ä¿å­˜
                try:
                    if cache_type == 'documents':
                        with open(cache_file, 'w', encoding='utf-8') as f:
                            json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    else:
                        with open(cache_file, 'wb') as f:
                            pickle.dump(cache_data, f)
                    print(f"âœ… ä¿å­˜ {cache_type} å¿«å–: {len(cache_data)} é …ç›®")
                except Exception as e:
                    print(f"âŒ ä¿å­˜ {cache_type} å¿«å–å¤±æ•—: {e}")
    
    def extract_main_content(self, raw_content: str) -> str:
        """å¾åŸå§‹å…§å®¹ä¸­æå–ä¸»è¦æ–‡ç« å…§å®¹ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
        # å¿«é€Ÿæª¢æŸ¥åˆ†éš”ç·š
        separator = '--------------------------------------------------------------------------------'
        separator_index = raw_content.find(separator)
        
        if separator_index != -1:
            # æ‰¾åˆ°åˆ†éš”ç·šï¼Œæå–å¾Œé¢çš„å…§å®¹
            main_content = raw_content[separator_index + len(separator):].strip()
        else:
            # æ²’æœ‰åˆ†éš”ç·šï¼Œè¿”å›å…¨éƒ¨å…§å®¹
            main_content = raw_content
        
        return main_content
    
    def preprocess_text(self, text: str, for_semantic: bool = False) -> str:
        """
        æ–‡å­—å‰è™•ç†ï¼ˆå„ªåŒ–ç‰ˆï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            for_semantic: æ˜¯å¦ç‚ºèªç¾©åˆ†ææº–å‚™
        """
        # æå–ä¸»è¦å…§å®¹
        text = self.extract_main_content(text)
        
        # åŸºæœ¬æ¸…ç†ï¼ˆä½¿ç”¨ç·¨è­¯çš„æ­£å‰‡è¡¨é”å¼æœƒæ›´å¿«ï¼‰
        if not hasattr(self, '_compiled_regexes'):
            self._compiled_regexes = {
                'html': re.compile(r'<[^>]+>'),
                'url': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'),
                'email': re.compile(r'\S+@\S+'),
                'non_word': re.compile(r'[^\w\s]'),
                'non_alpha': re.compile(r'[^a-zA-Z\s]'),
                'whitespace': re.compile(r'\s+')
            }
        
        # å»é™¤HTMLã€URLã€email
        text = self._compiled_regexes['html'].sub('', text)
        text = self._compiled_regexes['url'].sub('', text)
        text = self._compiled_regexes['email'].sub('', text)
        
        if for_semantic:
            # èªç¾©åˆ†æï¼šä¿ç•™æ›´å¤šåŸå§‹çµæ§‹
            text = self._compiled_regexes['non_word'].sub(' ', text)
            text = self._compiled_regexes['whitespace'].sub(' ', text).strip()
            return text
        
        # TF-IDFåˆ†æï¼šå®Œæ•´å‰è™•ç†
        text = text.lower()
        text = self._compiled_regexes['non_alpha'].sub(' ', text)
        text = self._compiled_regexes['whitespace'].sub(' ', text).strip()
        
        # åˆ†è©å’Œéæ¿¾ï¼ˆå‘é‡åŒ–è™•ç†ï¼‰
        tokens = self.tokenizer.tokenize(text)
        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]
        
        # è©å¹¹é‚„åŸï¼ˆæ‰¹æ¬¡è™•ç†æ›´æœ‰æ•ˆç‡ï¼‰
        if tokens:  # åªæœ‰ç•¶æœ‰tokenæ™‚æ‰é€²è¡Œè©å¹¹é‚„åŸ
            tokens = [self.stemmer.stem(token) for token in tokens]
        
        return ' '.join(tokens)
    
    def process_file(self, file_path: str, force_update: bool = False) -> Optional[Dict]:
        """
        è™•ç†å–®å€‹æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾‘
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°ï¼ˆå¿½ç•¥å¿«å–ï¼‰
            
        Returns:
            æ–‡æª”è™•ç†çµæœå­—å…¸
        """
        file_path = str(file_path)
        file_hash = self._get_file_hash(file_path)
        
        # æª¢æŸ¥å¿«å–
        if not force_update and file_path in self.document_cache:
            cached_info = self.document_cache[file_path]
            if cached_info.get('file_hash') == file_hash:
                return cached_info
        
        # è®€å–æ–‡ä»¶
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_content = f.read()
        except Exception as e:
            print(f"âŒ è®€å–æ–‡ä»¶å¤±æ•— {file_path}: {e}")
            return None
        
        # è™•ç†æ–‡æœ¬
        main_content = self.extract_main_content(raw_content)
        tfidf_text = self.preprocess_text(raw_content, for_semantic=False)
        semantic_text = self.preprocess_text(raw_content, for_semantic=True) if self.use_semantic else None
        
        # å»ºç«‹æ–‡æª”è³‡è¨Š
        doc_info = {
            'file_path': file_path,
            'file_hash': file_hash,
            'file_size': len(raw_content),
            'main_content_length': len(main_content),
            'tfidf_text': tfidf_text,
            'tfidf_word_count': len(tfidf_text.split()) if tfidf_text else 0,
            'semantic_text': semantic_text,
            'semantic_word_count': len(semantic_text.split()) if semantic_text else 0,
            'processed_time': datetime.now().isoformat()
        }
        
        # æ›´æ–°å¿«å–
        self.document_cache[file_path] = doc_info
        
        return doc_info
    
    def build_tfidf_vectors(self, file_paths: List[str], force_update: bool = False) -> Dict:
        """
        å»ºç«‹TF-IDFå‘é‡è³‡æ–™åº«
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°
            
        Returns:
            TF-IDFè³‡æ–™åº«å­—å…¸
        """
        print("ğŸ”„ å»ºç«‹TF-IDFå‘é‡è³‡æ–™åº«...")
        start_time = time.time()
        
        # è™•ç†æ‰€æœ‰æ–‡æª”
        documents = []
        valid_paths = []
        
        for file_path in file_paths:
            doc_info = self.process_file(file_path, force_update)
            if doc_info and doc_info['tfidf_text']:
                documents.append(doc_info['tfidf_text'])
                valid_paths.append(file_path)
        
        if not documents:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æª”å¯ä»¥è™•ç†")
            return {}
        
        # å»ºç«‹TF-IDFå‘é‡
        print(f"   è™•ç† {len(documents)} å€‹æ–‡æª”...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,  # é™åˆ¶ç‰¹å¾µæ•¸é‡ä»¥æå‡æ€§èƒ½
            min_df=2,  # å¿½ç•¥å‡ºç¾æ¬¡æ•¸å¤ªå°‘çš„è©
            max_df=0.95,  # å¿½ç•¥å‡ºç¾æ¬¡æ•¸å¤ªå¤šçš„è©
            ngram_range=(1, 2)  # è€ƒæ…®1-gramå’Œ2-gram
        )
        
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        
        # å»ºç«‹å‘é‡è³‡æ–™åº«
        tfidf_db = {
            'vectorizer': self.tfidf_vectorizer,
            'matrix': tfidf_matrix,
            'feature_names': feature_names,
            'file_paths': valid_paths,
            'vocab_size': len(feature_names),
            'created_time': datetime.now().isoformat()
        }
        
        # æ›´æ–°å¿«å–
        self.tfidf_cache = tfidf_db
        
        processing_time = time.time() - start_time
        print(f"âœ… TF-IDFå‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")
        print(f"   æ–‡æª”æ•¸é‡: {len(valid_paths)}")
        print(f"   è©å½™è¡¨å¤§å°: {len(feature_names)}")
        print(f"   çŸ©é™£å½¢ç‹€: {tfidf_matrix.shape}")
        print(f"   è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
        
        return tfidf_db
    
    def build_semantic_vectors(self, file_paths: List[str], force_update: bool = False) -> Dict:
        """
        å»ºç«‹èªç¾©å‘é‡è³‡æ–™åº«
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
            force_update: æ˜¯å¦å¼·åˆ¶æ›´æ–°
            
        Returns:
            èªç¾©å‘é‡è³‡æ–™åº«å­—å…¸
        """
        if not self.use_semantic or not self.semantic_model:
            print("âš ï¸  èªç¾©å‘é‡åŠŸèƒ½æœªå•Ÿç”¨")
            return {}
        
        print("ğŸ”„ å»ºç«‹èªç¾©å‘é‡è³‡æ–™åº«...")
        start_time = time.time()
        
        # è™•ç†æ‰€æœ‰æ–‡æª”
        documents = []
        valid_paths = []
        
        for file_path in file_paths:
            doc_info = self.process_file(file_path, force_update)
            if doc_info and doc_info['semantic_text']:
                documents.append(doc_info['semantic_text'])
                valid_paths.append(file_path)
        
        if not documents:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æª”å¯ä»¥è™•ç†")
            return {}
        
        # å»ºç«‹èªç¾©å‘é‡ï¼ˆæ‰¹æ¬¡è™•ç†æå‡æ•ˆç‡ï¼‰
        print(f"   è™•ç† {len(documents)} å€‹æ–‡æª”...")
        batch_size = 32  # æ‰¹æ¬¡å¤§å°ï¼Œé¿å…è¨˜æ†¶é«”ä¸è¶³
        all_embeddings = []
        
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i+batch_size]
            batch_embeddings = self.semantic_model.encode(
                batch_docs,
                show_progress_bar=False,
                batch_size=batch_size
            )
            all_embeddings.append(batch_embeddings)
        
        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        embeddings = np.vstack(all_embeddings)
        
        # å»ºç«‹å‘é‡è³‡æ–™åº«
        semantic_db = {
            'model_name': 'all-MiniLM-L6-v2',
            'embeddings': embeddings,
            'file_paths': valid_paths,
            'vector_dim': embeddings.shape[1],
            'created_time': datetime.now().isoformat()
        }
        
        # æ›´æ–°å¿«å–
        self.semantic_cache = semantic_db
        
        processing_time = time.time() - start_time
        print(f"âœ… èªç¾©å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")
        print(f"   æ–‡æª”æ•¸é‡: {len(valid_paths)}")
        print(f"   å‘é‡ç¶­åº¦: {embeddings.shape[1]}")
        print(f"   çŸ©é™£å½¢ç‹€: {embeddings.shape}")
        print(f"   è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
        
        return semantic_db
    
    def find_similar_documents(self, query_file: str, method: str = 'both', top_k: int = 5) -> Dict:
        """
        æŸ¥æ‰¾ç›¸ä¼¼æ–‡æª”
        
        Args:
            query_file: æŸ¥è©¢æ–‡ä»¶è·¯å¾‘
            method: 'tfidf', 'semantic', æˆ– 'both'
            top_k: è¿”å›å‰kå€‹æœ€ç›¸ä¼¼çš„æ–‡æª”
            
        Returns:
            ç›¸ä¼¼åº¦çµæœå­—å…¸
        """
        results = {}
        
        # è™•ç†æŸ¥è©¢æ–‡æª”
        query_doc = self.process_file(query_file)
        if not query_doc:
            return {'error': f'ç„¡æ³•è™•ç†æŸ¥è©¢æ–‡æª”: {query_file}'}
        
        # TF-IDFç›¸ä¼¼åº¦
        if method in ['tfidf', 'both'] and self.tfidf_cache:
            tfidf_db = self.tfidf_cache
            if query_doc['tfidf_text']:
                # å°‡æŸ¥è©¢æ–‡æª”è½‰æ›ç‚ºå‘é‡
                query_vector = tfidf_db['vectorizer'].transform([query_doc['tfidf_text']])
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarities = cosine_similarity(query_vector, tfidf_db['matrix']).flatten()
                
                # ç²å–å‰kå€‹æœ€ç›¸ä¼¼çš„æ–‡æª”
                top_indices = similarities.argsort()[-top_k-1:-1][::-1]  # æ’é™¤è‡ªå·±
                
                tfidf_results = []
                for idx in top_indices:
                    if similarities[idx] > 0:  # åªåŒ…å«æœ‰ç›¸ä¼¼åº¦çš„æ–‡æª”
                        tfidf_results.append({
                            'file_path': tfidf_db['file_paths'][idx],
                            'similarity': float(similarities[idx]),
                            'rank': len(tfidf_results) + 1
                        })
                
                results['tfidf'] = tfidf_results
        
        # èªç¾©ç›¸ä¼¼åº¦
        if method in ['semantic', 'both'] and self.semantic_cache and self.semantic_model:
            semantic_db = self.semantic_cache
            if query_doc['semantic_text']:
                # å°‡æŸ¥è©¢æ–‡æª”è½‰æ›ç‚ºå‘é‡
                query_embedding = self.semantic_model.encode([query_doc['semantic_text']])
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarities = cosine_similarity(query_embedding, semantic_db['embeddings']).flatten()
                
                # ç²å–å‰kå€‹æœ€ç›¸ä¼¼çš„æ–‡æª”
                top_indices = similarities.argsort()[-top_k-1:-1][::-1]  # æ’é™¤è‡ªå·±
                
                semantic_results = []
                for idx in top_indices:
                    if similarities[idx] > 0:  # åªåŒ…å«æœ‰ç›¸ä¼¼åº¦çš„æ–‡æª”
                        semantic_results.append({
                            'file_path': semantic_db['file_paths'][idx],
                            'similarity': float(similarities[idx]),
                            'rank': len(semantic_results) + 1
                        })
                
                results['semantic'] = semantic_results
        
        return results
    
    def get_statistics(self) -> Dict:
        """ç²å–è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'documents': {
                'total': len(self.document_cache),
                'processed': len([d for d in self.document_cache.values() if d.get('tfidf_text')])
            },
            'tfidf': {
                'available': bool(self.tfidf_cache),
                'documents': len(self.tfidf_cache.get('file_paths', [])),
                'vocab_size': self.tfidf_cache.get('vocab_size', 0)
            },
            'semantic': {
                'available': bool(self.semantic_cache),
                'documents': len(self.semantic_cache.get('file_paths', [])),
                'vector_dim': self.semantic_cache.get('vector_dim', 0)
            }
        }
        return stats
    
    def cleanup_cache(self):
        """æ¸…ç†å¿«å–ï¼ˆä¿å­˜åˆ°ç£ç¢Ÿï¼‰"""
        self._save_cache()
        print("âœ… å¿«å–å·²ä¿å­˜åˆ°ç£ç¢Ÿ")

def main():
    parser = argparse.ArgumentParser(description='æ–‡æœ¬é è™•ç†å’Œå‘é‡è³‡æ–™åº«å»ºç«‹ç¨‹å¼')
    parser.add_argument('--input-dir', type=str, required=True,
                      help='è¼¸å…¥æ–‡ä»¶ç›®éŒ„')
    parser.add_argument('--output-dir', type=str, default='vector_cache',
                      help='è¼¸å‡ºå‘é‡å¿«å–ç›®éŒ„ (é è¨­: vector_cache)')
    parser.add_argument('--method', type=str, choices=['tfidf', 'semantic', 'both'], default='both',
                      help='å‘é‡åŒ–æ–¹æ³• (é è¨­: both)')
    parser.add_argument('--pattern', type=str, default='*.txt',
                      help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é è¨­: *.txt)')
    parser.add_argument('--force-update', action='store_true',
                      help='å¼·åˆ¶æ›´æ–°æ‰€æœ‰å¿«å–')
    parser.add_argument('--query', type=str,
                      help='æŸ¥è©¢æ–‡ä»¶è·¯å¾‘ï¼ˆç”¨æ–¼ç›¸ä¼¼åº¦æœå°‹ï¼‰')
    parser.add_argument('--top-k', type=int, default=5,
                      help='è¿”å›å‰kå€‹æœ€ç›¸ä¼¼æ–‡æª” (é è¨­: 5)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å‘é‡åŒ–å™¨
    use_semantic = args.method in ['semantic', 'both']
    vectorizer = DocumentVectorizer(cache_dir=args.output_dir, use_semantic=use_semantic)
    
    # ç²å–è¼¸å…¥æ–‡ä»¶
    input_path = Path(args.input_dir)
    if not input_path.exists():
        print(f"âŒ è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    file_paths = list(input_path.glob(args.pattern))
    file_paths = [str(p) for p in file_paths if p.is_file()]
    
    if not file_paths:
        print(f"âŒ åœ¨ {args.input_dir} ä¸­æ‰¾ä¸åˆ°åŒ¹é… {args.pattern} çš„æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(file_paths)} å€‹æ–‡ä»¶")
    
    # å¦‚æœæ˜¯æŸ¥è©¢æ¨¡å¼
    if args.query:
        if not Path(args.query).exists():
            print(f"âŒ æŸ¥è©¢æ–‡ä»¶ä¸å­˜åœ¨: {args.query}")
            return
        
        print(f"\nğŸ” æœå°‹èˆ‡ {args.query} ç›¸ä¼¼çš„æ–‡æª”...")
        results = vectorizer.find_similar_documents(args.query, args.method, args.top_k)
        
        if 'error' in results:
            print(f"âŒ {results['error']}")
            return
        
        for method_name, method_results in results.items():
            print(f"\n{method_name.upper()} ç›¸ä¼¼åº¦çµæœ:")
            for result in method_results:
                print(f"  {result['rank']}. {result['file_path']} (ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
    
    else:
        # å»ºç«‹å‘é‡è³‡æ–™åº«
        if args.method in ['tfidf', 'both']:
            vectorizer.build_tfidf_vectors(file_paths, args.force_update)
        
        if args.method in ['semantic', 'both']:
            vectorizer.build_semantic_vectors(file_paths, args.force_update)
        
        # ä¿å­˜å¿«å–
        vectorizer.cleanup_cache()
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = vectorizer.get_statistics()
        print(f"\nğŸ“Š è³‡æ–™åº«çµ±è¨ˆ:")
        print(f"   æ–‡æª”ç¸½æ•¸: {stats['documents']['total']}")
        print(f"   å·²è™•ç†æ–‡æª”: {stats['documents']['processed']}")
        if stats['tfidf']['available']:
            print(f"   TF-IDFæ–‡æª”æ•¸: {stats['tfidf']['documents']}")
            print(f"   TF-IDFè©å½™è¡¨: {stats['tfidf']['vocab_size']}")
        if stats['semantic']['available']:
            print(f"   èªç¾©å‘é‡æ–‡æª”æ•¸: {stats['semantic']['documents']}")
            print(f"   èªç¾©å‘é‡ç¶­åº¦: {stats['semantic']['vector_dim']}")

if __name__ == "__main__":
    main() 