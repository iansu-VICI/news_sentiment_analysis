#!/usr/bin/env python3
"""
å¿«é€Ÿé¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æç¨‹å¼ï¼ˆå„ªåŒ–ç‰ˆï¼‰
ä½¿ç”¨é è™•ç†å¥½çš„å‘é‡è³‡æ–™åº«é€²è¡Œå¿«é€Ÿç›¸ä¼¼åº¦è¨ˆç®—

åŠŸèƒ½ï¼š
1. ä½¿ç”¨é è™•ç†å¥½çš„å‘é‡è³‡æ–™åº«é€²è¡Œå¿«é€ŸæŸ¥è©¢
2. æ”¯æ´å³æ™‚æ–‡æª”åˆ†æï¼ˆä¸ä½¿ç”¨è³‡æ–™åº«ï¼‰
3. æ‰¹æ¬¡ç›¸ä¼¼åº¦è¨ˆç®—
4. å‘é‡è³‡æ–™åº«ç®¡ç†

ä½¿ç”¨æ–¹å¼ï¼š
# ä½¿ç”¨å‘é‡è³‡æ–™åº«æŸ¥è©¢
python cosine_similarity_analysis_fast.py --vector-db vector_cache/ --query 1.txt --method both

# å³æ™‚åˆ†æå…©ç¯‡æ–‡ç« 
python cosine_similarity_analysis_fast.py --file1 1.txt --file2 2.txt --method tfidf

# æ‰¹æ¬¡åˆ†æ
python cosine_similarity_analysis_fast.py --batch-analysis --input-dir articles/ --vector-db vector_cache/
"""

import os
import argparse
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# å°å…¥æˆ‘å€‘çš„é è™•ç†æ¨¡çµ„
from preprocessing import DocumentVectorizer

class FastSimilarityAnalyzer:
    """å¿«é€Ÿç›¸ä¼¼åº¦åˆ†æå™¨"""
    
    def __init__(self, vector_db_path: Optional[str] = None, use_semantic: bool = True):
        """
        åˆå§‹åŒ–å¿«é€Ÿåˆ†æå™¨
        
        Args:
            vector_db_path: å‘é‡è³‡æ–™åº«è·¯å¾‘
            use_semantic: æ˜¯å¦å•Ÿç”¨èªç¾©åˆ†æ
        """
        self.vector_db_path = vector_db_path
        self.vectorizer = None
        self.use_semantic = use_semantic
        
        if vector_db_path:
            print(f"ğŸ”„ è¼‰å…¥å‘é‡è³‡æ–™åº«: {vector_db_path}")
            start_time = time.time()
            self.vectorizer = DocumentVectorizer(cache_dir=vector_db_path, use_semantic=use_semantic)
            load_time = time.time() - start_time
            print(f"âœ… å‘é‡è³‡æ–™åº«è¼‰å…¥å®Œæˆ ({load_time:.2f}ç§’)")
            
            # é¡¯ç¤ºè³‡æ–™åº«çµ±è¨ˆ
            stats = self.vectorizer.get_statistics()
            self._print_db_stats(stats)
        else:
            # å³æ™‚æ¨¡å¼ï¼Œæ¯æ¬¡éƒ½åˆå§‹åŒ–
            print("âš ï¸  å³æ™‚åˆ†ææ¨¡å¼ï¼ˆè¼ƒæ…¢ï¼‰")
    
    def _print_db_stats(self, stats: Dict):
        """æ‰“å°è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        print(f"\nğŸ“Š å‘é‡è³‡æ–™åº«çµ±è¨ˆ:")
        print(f"   æ–‡æª”ç¸½æ•¸: {stats['documents']['total']}")
        print(f"   å·²è™•ç†æ–‡æª”: {stats['documents']['processed']}")
        
        if stats['tfidf']['available']:
            print(f"   âœ… TF-IDFè³‡æ–™åº«: {stats['tfidf']['documents']} æ–‡æª”, {stats['tfidf']['vocab_size']} è©å½™")
        else:
            print(f"   âŒ TF-IDFè³‡æ–™åº«: æœªå»ºç«‹")
            
        if stats['semantic']['available']:
            print(f"   âœ… èªç¾©å‘é‡è³‡æ–™åº«: {stats['semantic']['documents']} æ–‡æª”, {stats['semantic']['vector_dim']} ç¶­åº¦")
        else:
            print(f"   âŒ èªç¾©å‘é‡è³‡æ–™åº«: æœªå»ºç«‹")
    
    def analyze_similarity_fast(self, file1: str, file2: str, method: str = 'both') -> Dict:
        """
        å¿«é€Ÿè¨ˆç®—å…©å€‹æ–‡æª”çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨è³‡æ–™åº«ï¼‰
        
        Args:
            file1: ç¬¬ä¸€å€‹æ–‡æª”è·¯å¾‘
            file2: ç¬¬äºŒå€‹æ–‡æª”è·¯å¾‘
            method: 'tfidf', 'semantic', æˆ– 'both'
            
        Returns:
            ç›¸ä¼¼åº¦çµæœå­—å…¸
        """
        if not self.vectorizer:
            return {'error': 'å‘é‡è³‡æ–™åº«æœªè¼‰å…¥ï¼Œè«‹ä½¿ç”¨ --vector-db åƒæ•¸æŒ‡å®šè³‡æ–™åº«è·¯å¾‘'}
        
        print("=== å¿«é€Ÿé¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æ ===")
        print(f"æ–‡æª”1: {os.path.basename(file1)}")
        print(f"æ–‡æª”2: {os.path.basename(file2)}")
        print(f"åˆ†ææ–¹æ³•: {method}")
        print()
        
        start_time = time.time()
        results = {}
        
        # ç¢ºä¿å…©å€‹æ–‡æª”éƒ½åœ¨è³‡æ–™åº«ä¸­
        file1_abs = os.path.abspath(file1)
        file2_abs = os.path.abspath(file2)
        
        # TF-IDFåˆ†æ
        if method in ['tfidf', 'both'] and self.vectorizer.tfidf_cache:
            tfidf_db = self.vectorizer.tfidf_cache
            
            try:
                # æŸ¥æ‰¾æ–‡æª”åœ¨è³‡æ–™åº«ä¸­çš„ç´¢å¼•
                idx1 = tfidf_db['file_paths'].index(file1_abs)
                idx2 = tfidf_db['file_paths'].index(file2_abs)
                
                # ç²å–å‘é‡
                vector1 = tfidf_db['matrix'][idx1]
                vector2 = tfidf_db['matrix'][idx2]
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarity = cosine_similarity(vector1, vector2)[0][0]
                
                results['tfidf'] = {
                    'similarity': float(similarity),
                    'method': 'TF-IDF (å¿«å–)',
                    'vocab_size': len(tfidf_db['feature_names'])
                }
                
                print(f"ğŸ“Š TF-IDFåˆ†æ:")
                print(f"   è©å½™è¡¨å¤§å°: {len(tfidf_db['feature_names'])}")
                print(f"   ç›¸ä¼¼åº¦: {similarity:.6f}")
                
            except ValueError as e:
                print(f"âš ï¸  TF-IDFåˆ†æå¤±æ•—: æ–‡æª”ä¸åœ¨è³‡æ–™åº«ä¸­")
                # å›é€€åˆ°å³æ™‚è¨ˆç®—
                results['tfidf'] = self._fallback_tfidf_analysis(file1, file2)
        
        # èªç¾©åˆ†æ
        if method in ['semantic', 'both'] and self.vectorizer.semantic_cache:
            semantic_db = self.vectorizer.semantic_cache
            
            try:
                # æŸ¥æ‰¾æ–‡æª”åœ¨è³‡æ–™åº«ä¸­çš„ç´¢å¼•
                idx1 = semantic_db['file_paths'].index(file1_abs)
                idx2 = semantic_db['file_paths'].index(file2_abs)
                
                # ç²å–å‘é‡
                vector1 = semantic_db['embeddings'][idx1:idx1+1]
                vector2 = semantic_db['embeddings'][idx2:idx2+1]
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarity = cosine_similarity(vector1, vector2)[0][0]
                
                results['semantic'] = {
                    'similarity': float(similarity),
                    'method': 'èªç¾©å‘é‡ (å¿«å–)',
                    'vector_dim': semantic_db['vector_dim']
                }
                
                print(f"ğŸ“Š èªç¾©å‘é‡åˆ†æ:")
                print(f"   å‘é‡ç¶­åº¦: {semantic_db['vector_dim']}")
                print(f"   ç›¸ä¼¼åº¦: {similarity:.6f}")
                
            except ValueError as e:
                print(f"âš ï¸  èªç¾©åˆ†æå¤±æ•—: æ–‡æª”ä¸åœ¨è³‡æ–™åº«ä¸­")
                # å›é€€åˆ°å³æ™‚è¨ˆç®—
                if self.vectorizer.semantic_model:
                    results['semantic'] = self._fallback_semantic_analysis(file1, file2)
        
        processing_time = time.time() - start_time
        results['processing_time'] = processing_time
        
        print(f"\nâš¡ è™•ç†æ™‚é–“: {processing_time:.3f}ç§’")
        
        # é¡¯ç¤ºæœ€çµ‚çµæœ
        self._print_similarity_results(results)
        
        return results
    
    def _fallback_tfidf_analysis(self, file1: str, file2: str) -> Dict:
        """å›é€€åˆ°å³æ™‚TF-IDFè¨ˆç®—"""
        print("ğŸ”„ å›é€€åˆ°å³æ™‚TF-IDFè¨ˆç®—...")
        
        # ä½¿ç”¨ç¾æœ‰çš„å‘é‡åŒ–å™¨é€²è¡Œå³æ™‚è¨ˆç®—
        doc1_info = self.vectorizer.process_file(file1)
        doc2_info = self.vectorizer.process_file(file2)
        
        if not doc1_info or not doc2_info:
            return {'error': 'ç„¡æ³•è™•ç†æ–‡æª”'}
        
        # ä½¿ç”¨ç¾æœ‰çš„TF-IDFå‘é‡åŒ–å™¨æˆ–å‰µå»ºæ–°çš„
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        documents = [doc1_info['tfidf_text'], doc2_info['tfidf_text']]
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(documents)
        
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        
        return {
            'similarity': float(similarity),
            'method': 'TF-IDF (å³æ™‚)',
            'vocab_size': len(vectorizer.get_feature_names_out())
        }
    
    def _fallback_semantic_analysis(self, file1: str, file2: str) -> Dict:
        """å›é€€åˆ°å³æ™‚èªç¾©åˆ†æ"""
        print("ğŸ”„ å›é€€åˆ°å³æ™‚èªç¾©åˆ†æ...")
        
        doc1_info = self.vectorizer.process_file(file1)
        doc2_info = self.vectorizer.process_file(file2)
        
        if not doc1_info or not doc2_info:
            return {'error': 'ç„¡æ³•è™•ç†æ–‡æª”'}
        
        # ä½¿ç”¨èªç¾©æ¨¡å‹
        embeddings = self.vectorizer.semantic_model.encode([
            doc1_info['semantic_text'],
            doc2_info['semantic_text']
        ])
        
        similarity = cosine_similarity(embeddings[0:1], embeddings[1:2])[0][0]
        
        return {
            'similarity': float(similarity),
            'method': 'èªç¾©å‘é‡ (å³æ™‚)',
            'vector_dim': embeddings.shape[1]
        }
    
    def analyze_similarity_realtime(self, file1: str, file2: str, method: str = 'both') -> Dict:
        """
        å³æ™‚è¨ˆç®—å…©å€‹æ–‡æª”çš„ç›¸ä¼¼åº¦ï¼ˆä¸ä½¿ç”¨è³‡æ–™åº«ï¼‰
        
        Args:
            file1: ç¬¬ä¸€å€‹æ–‡æª”è·¯å¾‘
            file2: ç¬¬äºŒå€‹æ–‡æª”è·¯å¾‘
            method: 'tfidf', 'semantic', æˆ– 'both'
            
        Returns:
            ç›¸ä¼¼åº¦çµæœå­—å…¸
        """
        print("=== å³æ™‚é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æ ===")
        print(f"æ–‡æª”1: {os.path.basename(file1)}")
        print(f"æ–‡æª”2: {os.path.basename(file2)}")
        print(f"åˆ†ææ–¹æ³•: {method}")
        print()
        
        start_time = time.time()
        
        # åˆå§‹åŒ–å‘é‡åŒ–å™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if not self.vectorizer:
            self.vectorizer = DocumentVectorizer(use_semantic=self.use_semantic)
        
        results = {}
        
        # TF-IDFåˆ†æ
        if method in ['tfidf', 'both']:
            results['tfidf'] = self._fallback_tfidf_analysis(file1, file2)
        
        # èªç¾©åˆ†æ
        if method in ['semantic', 'both'] and self.vectorizer.semantic_model:
            results['semantic'] = self._fallback_semantic_analysis(file1, file2)
        
        processing_time = time.time() - start_time
        results['processing_time'] = processing_time
        
        print(f"\nâš¡ è™•ç†æ™‚é–“: {processing_time:.3f}ç§’")
        
        # é¡¯ç¤ºæœ€çµ‚çµæœ
        self._print_similarity_results(results)
        
        return results
    
    def query_similar_documents(self, query_file: str, method: str = 'both', top_k: int = 5) -> Dict:
        """
        æŸ¥è©¢èˆ‡æŒ‡å®šæ–‡æª”ç›¸ä¼¼çš„å…¶ä»–æ–‡æª”
        
        Args:
            query_file: æŸ¥è©¢æ–‡æª”è·¯å¾‘
            method: åˆ†ææ–¹æ³•
            top_k: è¿”å›å‰kå€‹çµæœ
            
        Returns:
            æŸ¥è©¢çµæœå­—å…¸
        """
        if not self.vectorizer:
            return {'error': 'å‘é‡è³‡æ–™åº«æœªè¼‰å…¥'}
        
        print(f"ğŸ” æŸ¥è©¢èˆ‡ {os.path.basename(query_file)} ç›¸ä¼¼çš„æ–‡æª”...")
        print(f"æ–¹æ³•: {method}, è¿”å›å‰ {top_k} å€‹çµæœ")
        print()
        
        start_time = time.time()
        results = self.vectorizer.find_similar_documents(query_file, method, top_k)
        processing_time = time.time() - start_time
        
        print(f"âš¡ æŸ¥è©¢æ™‚é–“: {processing_time:.3f}ç§’")
        
        if 'error' in results:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {results['error']}")
            return results
        
        # é¡¯ç¤ºçµæœ
        for method_name, method_results in results.items():
            print(f"\nğŸ“Š {method_name.upper()} ç›¸ä¼¼åº¦çµæœ:")
            if method_results:
                for result in method_results:
                    file_name = os.path.basename(result['file_path'])
                    print(f"  {result['rank']}. {file_name} (ç›¸ä¼¼åº¦: {result['similarity']:.4f})")
            else:
                print("  ç„¡ç›¸ä¼¼æ–‡æª”")
        
        return results
    
    def batch_analysis(self, input_dir: str, pattern: str = "*.txt", method: str = 'both') -> Dict:
        """
        æ‰¹æ¬¡åˆ†æç›®éŒ„ä¸­æ‰€æœ‰æ–‡æª”çš„ç›¸ä¼¼åº¦
        
        Args:
            input_dir: è¼¸å…¥ç›®éŒ„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            method: åˆ†ææ–¹æ³•
            
        Returns:
            æ‰¹æ¬¡åˆ†æçµæœ
        """
        if not self.vectorizer:
            return {'error': 'å‘é‡è³‡æ–™åº«æœªè¼‰å…¥'}
        
        print(f"ğŸ”„ æ‰¹æ¬¡åˆ†æ: {input_dir}")
        print(f"æ–‡ä»¶æ¨¡å¼: {pattern}")
        print(f"åˆ†ææ–¹æ³•: {method}")
        print()
        
        input_path = Path(input_dir)
        if not input_path.exists():
            return {'error': f'ç›®éŒ„ä¸å­˜åœ¨: {input_dir}'}
        
        file_paths = list(input_path.glob(pattern))
        file_paths = [str(p) for p in file_paths if p.is_file()]
        
        if len(file_paths) < 2:
            return {'error': f'éœ€è¦è‡³å°‘2å€‹æ–‡ä»¶é€²è¡Œæ‰¹æ¬¡åˆ†æ'}
        
        print(f"æ‰¾åˆ° {len(file_paths)} å€‹æ–‡ä»¶")
        
        start_time = time.time()
        results = []
        
        # è¨ˆç®—æ‰€æœ‰æ–‡æª”å°çš„ç›¸ä¼¼åº¦
        for i, file1 in enumerate(file_paths):
            for j, file2 in enumerate(file_paths[i+1:], i+1):
                similarity_result = self.analyze_similarity_fast(file1, file2, method)
                
                if 'error' not in similarity_result:
                    pair_result = {
                        'file1': os.path.basename(file1),
                        'file2': os.path.basename(file2),
                        'file1_path': file1,
                        'file2_path': file2,
                    }
                    
                    # æ·»åŠ å„ç¨®æ–¹æ³•çš„çµæœ
                    for method_name in ['tfidf', 'semantic']:
                        if method_name in similarity_result:
                            pair_result[f'{method_name}_similarity'] = similarity_result[method_name]['similarity']
                    
                    results.append(pair_result)
        
        processing_time = time.time() - start_time
        
        print(f"\nâœ… æ‰¹æ¬¡åˆ†æå®Œæˆ")
        print(f"   æ–‡æª”å°æ•¸: {len(results)}")
        print(f"   ç¸½è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"   å¹³å‡æ¯å°: {processing_time/len(results):.3f}ç§’")
        
        # æ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„æ–‡æª”å°
        if results:
            self._print_batch_summary(results, method)
        
        return {
            'results': results,
            'total_pairs': len(results),
            'processing_time': processing_time,
            'avg_time_per_pair': processing_time / len(results) if results else 0
        }
    
    def _print_similarity_results(self, results: Dict):
        """æ‰“å°ç›¸ä¼¼åº¦çµæœæ‘˜è¦"""
        print("\n=== ç›¸ä¼¼åº¦åˆ†æçµæœ ===")
        
        for method_name, result in results.items():
            if method_name == 'processing_time':
                continue
            if 'error' in result:
                print(f"âŒ {method_name}: {result['error']}")
                continue
            
            similarity = result['similarity']
            method_desc = result['method']
            
            print(f"ğŸ“Š {method_desc}: {similarity:.6f}")
            
            # ç›¸ä¼¼åº¦è§£é‡‹
            if similarity >= 0.8:
                interpretation = "éå¸¸é«˜ç›¸ä¼¼åº¦ - å¯èƒ½æ˜¯é‡è¤‡æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ç« "
                emoji = "ğŸ”´"
            elif similarity >= 0.6:
                interpretation = "é«˜ç›¸ä¼¼åº¦ - æ–‡ç« å…§å®¹ç›¸é—œæ€§å¾ˆé«˜"
                emoji = "ğŸŸ "
            elif similarity >= 0.4:
                interpretation = "ä¸­ç­‰ç›¸ä¼¼åº¦ - æ–‡ç« æœ‰ä¸€å®šç›¸é—œæ€§"
                emoji = "ğŸŸ¡"
            elif similarity >= 0.2:
                interpretation = "ä½ç›¸ä¼¼åº¦ - æ–‡ç« ç›¸é—œæ€§è¼ƒä½"
                emoji = "ğŸŸ¢"
            else:
                interpretation = "å¾ˆä½ç›¸ä¼¼åº¦ - æ–‡ç« å…§å®¹åŸºæœ¬ä¸ç›¸é—œ"
                emoji = "ğŸ”µ"
            
            print(f"   {emoji} {interpretation}")
            
            # æ ¹æ“šè«–æ–‡ä¸­çš„0.8é–¾å€¼åˆ¤æ–·
            if similarity >= 0.8:
                print(f"   ğŸ“‹ è«–æ–‡åˆ¤æ–·: é€™å…©ç¯‡æ–‡ç« æœƒè¢«æ¨™è¨˜ç‚ºé‡è¤‡ä¸¦æ¨æ£„")
            else:
                print(f"   ğŸ“‹ è«–æ–‡åˆ¤æ–·: é€™å…©ç¯‡æ–‡ç« æœƒè¢«ä¿ç•™")
    
    def _print_batch_summary(self, results: List[Dict], method: str):
        """æ‰“å°æ‰¹æ¬¡åˆ†ææ‘˜è¦"""
        print(f"\nğŸ“ˆ æ‰¹æ¬¡åˆ†ææ‘˜è¦:")
        
        for method_name in ['tfidf', 'semantic']:
            if method in [method_name, 'both']:
                similarity_key = f'{method_name}_similarity'
                similarities = [r[similarity_key] for r in results if similarity_key in r]
                
                if similarities:
                    max_sim = max(similarities)
                    min_sim = min(similarities)
                    avg_sim = sum(similarities) / len(similarities)
                    
                    print(f"\n{method_name.upper()} çµ±è¨ˆ:")
                    print(f"   å¹³å‡ç›¸ä¼¼åº¦: {avg_sim:.4f}")
                    print(f"   æœ€é«˜ç›¸ä¼¼åº¦: {max_sim:.4f}")
                    print(f"   æœ€ä½ç›¸ä¼¼åº¦: {min_sim:.4f}")
                    
                    # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„æ–‡æª”å°
                    max_pair = max(results, key=lambda x: x.get(similarity_key, 0))
                    min_pair = min(results, key=lambda x: x.get(similarity_key, 1))
                    
                    print(f"   æœ€ç›¸ä¼¼æ–‡æª”å°: {max_pair['file1']} â†” {max_pair['file2']} ({max_sim:.4f})")
                    print(f"   æœ€ä¸ç›¸ä¼¼æ–‡æª”å°: {min_pair['file1']} â†” {min_pair['file2']} ({min_sim:.4f})")
                    
                    # é«˜ç›¸ä¼¼åº¦è­¦å‘Š
                    high_similarity_pairs = [r for r in results if r.get(similarity_key, 0) >= 0.8]
                    if high_similarity_pairs:
                        print(f"   âš ï¸  é«˜ç›¸ä¼¼åº¦è­¦å‘Š: {len(high_similarity_pairs)} å°æ–‡æª”ç›¸ä¼¼åº¦ â‰¥ 0.8")

def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿé¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æç¨‹å¼ï¼ˆå„ªåŒ–ç‰ˆï¼‰')
    
    # è¼¸å…¥æ¨¡å¼
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--file1', type=str, help='ç¬¬ä¸€å€‹æ–‡æª”è·¯å¾‘ï¼ˆé…åˆ--file2ä½¿ç”¨ï¼‰')
    input_group.add_argument('--query', type=str, help='æŸ¥è©¢æ–‡æª”è·¯å¾‘ï¼ˆåœ¨è³‡æ–™åº«ä¸­æœå°‹ç›¸ä¼¼æ–‡æª”ï¼‰')
    input_group.add_argument('--batch-analysis', action='store_true', help='æ‰¹æ¬¡åˆ†ææ¨¡å¼')
    
    parser.add_argument('--file2', type=str, help='ç¬¬äºŒå€‹æ–‡æª”è·¯å¾‘ï¼ˆé…åˆ--file1ä½¿ç”¨ï¼‰')
    parser.add_argument('--vector-db', type=str, help='å‘é‡è³‡æ–™åº«è·¯å¾‘')
    parser.add_argument('--method', type=str, choices=['tfidf', 'semantic', 'both'], default='both',
                      help='åˆ†ææ–¹æ³• (é è¨­: both)')
    parser.add_argument('--top-k', type=int, default=5,
                      help='æŸ¥è©¢æ¨¡å¼ä¸‹è¿”å›å‰kå€‹çµæœ (é è¨­: 5)')
    
    # æ‰¹æ¬¡åˆ†æåƒæ•¸
    parser.add_argument('--input-dir', type=str, default='.',
                      help='æ‰¹æ¬¡åˆ†æè¼¸å…¥ç›®éŒ„ (é è¨­: ç•¶å‰ç›®éŒ„)')
    parser.add_argument('--pattern', type=str, default='*.txt',
                      help='æ‰¹æ¬¡åˆ†ææ–‡ä»¶åŒ¹é…æ¨¡å¼ (é è¨­: *.txt)')
    
    args = parser.parse_args()
    
    # é©—è­‰åƒæ•¸
    if args.file1 and not args.file2:
        parser.error("ä½¿ç”¨ --file1 æ™‚å¿…é ˆåŒæ™‚æŒ‡å®š --file2")
    
    if args.batch_analysis and not args.vector_db:
        parser.error("æ‰¹æ¬¡åˆ†ææ¨¡å¼éœ€è¦æŒ‡å®š --vector-db")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    use_semantic = args.method in ['semantic', 'both']
    analyzer = FastSimilarityAnalyzer(vector_db_path=args.vector_db, use_semantic=use_semantic)
    
    try:
        # åŸ·è¡Œç›¸æ‡‰çš„åˆ†æ
        if args.file1 and args.file2:
            # å…©æ–‡æª”æ¯”è¼ƒæ¨¡å¼
            if not os.path.exists(args.file1):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file1}")
                return
            if not os.path.exists(args.file2):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file2}")
                return
            
            if args.vector_db:
                result = analyzer.analyze_similarity_fast(args.file1, args.file2, args.method)
            else:
                result = analyzer.analyze_similarity_realtime(args.file1, args.file2, args.method)
                
        elif args.query:
            # æŸ¥è©¢æ¨¡å¼
            if not os.path.exists(args.query):
                print(f"âŒ æŸ¥è©¢æ–‡ä»¶ä¸å­˜åœ¨: {args.query}")
                return
            
            result = analyzer.query_similar_documents(args.query, args.method, args.top_k)
            
        elif args.batch_analysis:
            # æ‰¹æ¬¡åˆ†ææ¨¡å¼
            result = analyzer.batch_analysis(args.input_dir, args.pattern, args.method)
        
        # è™•ç†çµæœ
        if 'error' in result:
            print(f"âŒ åˆ†æå¤±æ•—: {result['error']}")
        else:
            print(f"\nâœ… åˆ†æå®Œæˆ")
            
    except KeyboardInterrupt:
        print(f"\nâŒ ç”¨æˆ¶ä¸­æ–·ç¨‹å¼")
    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 