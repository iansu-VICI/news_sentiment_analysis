#!/usr/bin/env python3
"""
æ ¹æ“š teach.md ä¸­çš„æ–¹æ³•è¨ˆç®—å…©ç¯‡æ–‡ç« çš„é¤˜å¼¦ç›¸ä¼¼åº¦
å¯¦ç¾æ­¥é©Ÿï¼š
1. æ–‡å­—å‰è™•ç†
2. TF-IDFå‘é‡åŒ–æˆ–å¥å‘é‡åµŒå…¥
3. è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦

å„ªåŒ–ç‰ˆæœ¬ï¼ˆåŸºæ–¼fix.mdå»ºè­°ï¼‰ï¼š
- ç§»é™¤ä¸å¿…è¦çš„punktæª¢æŸ¥ï¼Œé¿å…å¤šé¤˜çš„nltk.download()
- ä½¿ç”¨TweetTokenizeræ”¹å–„é‡‘èå°ˆæœ‰åè©çš„æ–·è©ç²¾ç¢ºåº¦
- ç§»é™¤æ‰‹å‹•å¯¦ç¾ï¼Œåªä¿ç•™é«˜æ•ˆçš„sklearnç‰ˆæœ¬
- æ”¯æŒsentence-transformersé€²è¡Œèªç¾©ç›¸ä¼¼åº¦åˆ†æ
"""

import re
import string
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import argparse
import os

# ä¸‹è¼‰å¿…è¦çš„NLTKæ•¸æ“šï¼ˆç§»é™¤punktæª¢æŸ¥ï¼Œæ ¹æ“šfix.mdå»ºè­°ï¼‰
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("ä¸‹è¼‰stopwordsæ•¸æ“š...")
    nltk.download('stopwords')

# å˜—è©¦å°å…¥sentence-transformersï¼ˆå¯é¸ï¼‰
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

class TextSimilarityAnalyzer:
    def __init__(self, use_semantic=False):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            use_semantic: æ˜¯å¦ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦ï¼ˆéœ€è¦sentence-transformersï¼‰
        """
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        # ä½¿ç”¨TweetTokenizeræ”¹å–„é‡‘èå°ˆæœ‰åè©è™•ç†ï¼ˆfix.mdå»ºè­°ï¼‰
        self.tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)
        
        self.use_semantic = use_semantic
        if use_semantic:
            if not SENTENCE_TRANSFORMERS_AVAILABLE:
                print("âš ï¸  sentence-transformersæœªå®‰è£ï¼Œå›é€€åˆ°TF-IDFæ–¹æ³•")
                print("   å®‰è£æ–¹æ³•: pip install sentence-transformers")
                self.use_semantic = False
            else:
                print("ğŸ”„ è¼‰å…¥èªç¾©å‘é‡æ¨¡å‹...")
                # ä½¿ç”¨é©åˆé‡‘èæ–‡æœ¬çš„æ¨¡å‹ï¼ˆfix.mdå»ºè­°ç”¨sentence-transformersï¼‰
                self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
                print("âœ… èªç¾©å‘é‡æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def read_article(self, file_path):
        """è®€å–æ–‡ç« æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content
        except Exception as e:
            print(f"è®€å–æ–‡ä»¶ {file_path} æ™‚å‡ºéŒ¯: {e}")
            return None
    
    def extract_main_content(self, raw_content):
        """å¾åŸå§‹å…§å®¹ä¸­æå–ä¸»è¦æ–‡ç« å…§å®¹"""
        lines = raw_content.split('\n')
        
        # æ‰¾åˆ°åˆ†éš”ç·šçš„ä½ç½®
        separator_line = -1
        for i, line in enumerate(lines):
            if line.strip() == '--------------------------------------------------------------------------------':
                separator_line = i
                break
        
        if separator_line == -1:
            # å¦‚æœæ²’æ‰¾åˆ°åˆ†éš”ç·šï¼Œè¿”å›å…¨éƒ¨å…§å®¹
            main_content = raw_content
        else:
            # æå–åˆ†éš”ç·šå¾Œçš„å…§å®¹ä½œç‚ºä¸»è¦æ–‡ç« å…§å®¹
            main_content = '\n'.join(lines[separator_line + 1:])
        
        return main_content
    
    def preprocess_text(self, text, for_semantic=False):
        """
        æ­¥é©Ÿ1: æ–‡å­—å‰è™•ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            for_semantic: æ˜¯å¦ç‚ºèªç¾©åˆ†ææº–å‚™ï¼ˆä¿ç•™æ›´å¤šåŸå§‹çµæ§‹ï¼‰
        """
        # æå–ä¸»è¦å…§å®¹
        text = self.extract_main_content(text)
        
        # å»é™¤HTMLæ¨™è¨˜
        text = re.sub(r'<[^>]+>', '', text)
        
        # å»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # å»é™¤email
        text = re.sub(r'\S+@\S+', '', text)
        
        if for_semantic:
            # èªç¾©åˆ†æï¼šä¿ç•™æ›´å¤šåŸå§‹çµæ§‹ï¼ŒåªåšåŸºæœ¬æ¸…ç†
            text = re.sub(r'[^\w\s]', ' ', text)  # ä¿ç•™å­—æ¯æ•¸å­—å’Œç©ºæ ¼
            text = re.sub(r'\s+', ' ', text).strip()
            return text
        
        # TF-IDFåˆ†æï¼šé€²è¡Œå®Œæ•´çš„å‰è™•ç†
        text = text.lower()
        
        # å»é™¤æ•¸å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        
        # å»é™¤å¤šé¤˜ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ä½¿ç”¨TweetTokenizeråˆ†è©ï¼ˆfix.mdå»ºè­°ï¼šæ”¹å–„é‡‘èå°ˆæœ‰åè©è™•ç†ï¼Œé¿å…word_tokenizeåˆ‡éŒ¯ï¼‰
        tokens = self.tokenizer.tokenize(text)
        
        # å»é™¤åœç”¨è©å’ŒçŸ­è©
        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]
        
        # è©å¹¹é‚„åŸ
        tokens = [self.stemmer.stem(token) for token in tokens]
        
        # è¿”å›è™•ç†å¾Œçš„æ–‡æœ¬
        return ' '.join(tokens)
    
    def analyze_similarity_tfidf(self, processed1, processed2):
        """ä½¿ç”¨TF-IDFæ–¹æ³•è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆfix.mdå»ºè­°ï¼šåªä¿ç•™sklearnç‰ˆæœ¬ï¼Œç§»é™¤æ‰‹å‹•å¯¦ç¾ï¼‰"""
        documents = [processed1, processed2]
        
        # ä½¿ç”¨scikit-learnçš„TF-IDFï¼ˆé«˜æ•ˆå¯¦ç¾ï¼Œé¿å…O(VÃ—NÃ—L)è¨˜æ†¶é«”å•é¡Œï¼‰
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(documents)
        feature_names = vectorizer.get_feature_names_out()
        
        print(f"è©å½™è¡¨å¤§å°: {len(feature_names)} å€‹è©")
        print(f"TF-IDFçŸ©é™£å½¢ç‹€: {tfidf_matrix.shape}")
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity_matrix = cosine_similarity(tfidf_matrix)
        cosine_sim = similarity_matrix[0][1]
        
        return cosine_sim, "TF-IDF"
    
    def analyze_similarity_semantic(self, text1, text2):
        """ä½¿ç”¨èªç¾©å‘é‡æ–¹æ³•è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆfix.mdå»ºè­°ï¼šæŠ“åˆ°èªç¾©è¿‘ä¼¼ï¼Œå…‹æœTF-IDFå°èªåºèªç¾©å¤±æ•ï¼‰"""
        # ç”Ÿæˆå¥å‘é‡ï¼ˆsentence-transformersæä¾›èªç¾©ç†è§£èƒ½åŠ›ï¼‰
        embeddings = self.semantic_model.encode([text1, text2])
        
        print(f"èªç¾©å‘é‡ç¶­åº¦: {embeddings.shape[1]}")
        print(f"å‘é‡çŸ©é™£å½¢ç‹€: {embeddings.shape}")
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆèªç¾©å±¤é¢çš„ç›¸ä¼¼åº¦ï¼‰
        similarity_matrix = cosine_similarity(embeddings)
        cosine_sim = similarity_matrix[0][1]
        
        return cosine_sim, "èªç¾©å‘é‡"
    
    def analyze_similarity(self, file1, file2):
        """åˆ†æå…©ç¯‡æ–‡ç« çš„ç›¸ä¼¼åº¦"""
        print("=== é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æï¼ˆå„ªåŒ–ç‰ˆï¼‰===")
        print(f"æ–‡ç« 1: {os.path.basename(file1)}")
        print(f"æ–‡ç« 2: {os.path.basename(file2)}")
        
        method = "èªç¾©å‘é‡" if self.use_semantic else "TF-IDF"
        print(f"åˆ†ææ–¹æ³•: {method}")
        print()
        
        # è®€å–æ–‡ç« 
        content1 = self.read_article(file1)
        content2 = self.read_article(file2)
        
        if not content1 or not content2:
            print("âŒ ç„¡æ³•è®€å–æ–‡ç« å…§å®¹")
            return
        
        print("æ­¥é©Ÿ1: æ–‡å­—å‰è™•ç†...")
        
        if self.use_semantic:
            # èªç¾©åˆ†æï¼šä¿ç•™æ›´å¤šçµæ§‹
            processed1 = self.preprocess_text(content1, for_semantic=True)
            processed2 = self.preprocess_text(content2, for_semantic=True)
            
            print(f"æ–‡ç« 1å‰è™•ç†å¾Œé•·åº¦: {len(processed1.split())} å€‹è©")
            print(f"æ–‡ç« 2å‰è™•ç†å¾Œé•·åº¦: {len(processed2.split())} å€‹è©")
            print()
            
            print("æ­¥é©Ÿ2: ç”Ÿæˆèªç¾©å‘é‡...")
            cosine_sim, method_used = self.analyze_similarity_semantic(processed1, processed2)
            
        else:
            # TF-IDFåˆ†æï¼šå®Œæ•´å‰è™•ç†
            processed1 = self.preprocess_text(content1, for_semantic=False)
            processed2 = self.preprocess_text(content2, for_semantic=False)
            
            print(f"æ–‡ç« 1å‰è™•ç†å¾Œé•·åº¦: {len(processed1.split())} å€‹è©")
            print(f"æ–‡ç« 2å‰è™•ç†å¾Œé•·åº¦: {len(processed2.split())} å€‹è©")
            print()
            
            print("æ­¥é©Ÿ2: TF-IDFå‘é‡åŒ–...")
            cosine_sim, method_used = self.analyze_similarity_tfidf(processed1, processed2)
        
        print("\næ­¥é©Ÿ3: è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦...")
        print()
        print("=== åˆ†æçµæœ ===")
        print(f"åˆ†ææ–¹æ³•: {method_used}")
        print(f"é¤˜å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
        
        # ç›¸ä¼¼åº¦è§£é‡‹
        if cosine_sim >= 0.8:
            interpretation = "éå¸¸é«˜ç›¸ä¼¼åº¦ - å¯èƒ½æ˜¯é‡è¤‡æˆ–é«˜åº¦ç›¸ä¼¼çš„æ–‡ç« "
        elif cosine_sim >= 0.6:
            interpretation = "é«˜ç›¸ä¼¼åº¦ - æ–‡ç« å…§å®¹ç›¸é—œæ€§å¾ˆé«˜"
        elif cosine_sim >= 0.4:
            interpretation = "ä¸­ç­‰ç›¸ä¼¼åº¦ - æ–‡ç« æœ‰ä¸€å®šç›¸é—œæ€§"
        elif cosine_sim >= 0.2:
            interpretation = "ä½ç›¸ä¼¼åº¦ - æ–‡ç« ç›¸é—œæ€§è¼ƒä½"
        else:
            interpretation = "å¾ˆä½ç›¸ä¼¼åº¦ - æ–‡ç« å…§å®¹åŸºæœ¬ä¸ç›¸é—œ"
        
        print(f"ç›¸ä¼¼åº¦è§£é‡‹: {interpretation}")
        
        # æ ¹æ“šè«–æ–‡ä¸­çš„0.8é–¾å€¼åˆ¤æ–·
        print(f"\næ ¹æ“šè«–æ–‡æ¨™æº– (é–¾å€¼ = 0.8):")
        if cosine_sim >= 0.8:
            print("ğŸ”„ é€™å…©ç¯‡æ–‡ç« æœƒè¢«æ¨™è¨˜ç‚ºé‡è¤‡ä¸¦æ¨æ£„")
        else:
            print("âœ… é€™å…©ç¯‡æ–‡ç« æœƒè¢«ä¿ç•™")
        
        # æ–¹æ³•æ¯”è¼ƒå»ºè­°
        if not self.use_semantic:
            print(f"\nğŸ’¡ æç¤º: å¦‚éœ€æ›´æº–ç¢ºçš„èªç¾©ç›¸ä¼¼åº¦åˆ†æï¼Œå¯å®‰è£ sentence-transformers:")
            print("   pip install sentence-transformers")
            print("   ç„¶å¾Œä½¿ç”¨ --semantic åƒæ•¸")
        
        return cosine_sim
    
    def show_preprocessing_details(self, file1, file2):
        """é¡¯ç¤ºå‰è™•ç†çš„è©³ç´°éç¨‹"""
        print("=== å‰è™•ç†è©³ç´°éç¨‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰===")
        
        for i, file_path in enumerate([file1, file2], 1):
            print(f"\næ–‡ç« {i}: {os.path.basename(file_path)}")
            content = self.read_article(file_path)
            if not content:
                continue
            
            # åŸå§‹å…§å®¹
            main_content = self.extract_main_content(content)
            print(f"åŸå§‹å…§å®¹é•·åº¦: {len(main_content)} å­—ç¬¦")
            print(f"åŸå§‹å…§å®¹å‰100å­—ç¬¦: {main_content[:100].replace(chr(10), ' ')}")
            
            # TF-IDFå‰è™•ç†
            processed_tfidf = self.preprocess_text(content, for_semantic=False)
            words_tfidf = processed_tfidf.split()
            print(f"TF-IDFå‰è™•ç†å¾Œè©æ•¸: {len(words_tfidf)} å€‹è©")
            print(f"TF-IDFå‰è™•ç†å¾Œå‰15å€‹è©: {' '.join(words_tfidf[:15])}")
            
            # èªç¾©åˆ†æå‰è™•ç†
            processed_semantic = self.preprocess_text(content, for_semantic=True)
            words_semantic = processed_semantic.split()
            print(f"èªç¾©åˆ†æå‰è™•ç†å¾Œè©æ•¸: {len(words_semantic)} å€‹è©")
            print(f"èªç¾©åˆ†æå‰è™•ç†å¾Œå‰15å€‹è©: {' '.join(words_semantic[:15])}")

def main():
    parser = argparse.ArgumentParser(description='è¨ˆç®—å…©ç¯‡æ–‡ç« çš„é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆå„ªåŒ–ç‰ˆï¼‰')
    parser.add_argument('file1', help='ç¬¬ä¸€ç¯‡æ–‡ç« çš„è·¯å¾‘')
    parser.add_argument('file2', help='ç¬¬äºŒç¯‡æ–‡ç« çš„è·¯å¾‘')
    parser.add_argument('--semantic', action='store_true', help='ä½¿ç”¨èªç¾©å‘é‡è€ŒéTF-IDFï¼ˆéœ€è¦sentence-transformersï¼‰')
    parser.add_argument('--details', action='store_true', help='é¡¯ç¤ºå‰è™•ç†çš„è©³ç´°éç¨‹')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.file1):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file1}")
        return
    if not os.path.exists(args.file2):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file2}")
        return
    
    analyzer = TextSimilarityAnalyzer(use_semantic=args.semantic)
    
    if args.details:
        analyzer.show_preprocessing_details(args.file1, args.file2)
        print("\n" + "="*50 + "\n")
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity = analyzer.analyze_similarity(args.file1, args.file2)

if __name__ == "__main__":
    main() 