#!/usr/bin/env python3
"""
æ‰¹é‡æ–°èä¸‹è¼‰å™¨ (å‡ç´šç‰ˆ) - ä½¿ç”¨ Crawl4AI å¹³è¡Œä¸‹è¼‰
-------------------------------------------------
1. é€é asyncio + Semaphore å¯¦ç¾ *è·¨æœˆä»½* å¹³è¡ŒæŠ“å–
2. ä¿ç•™ NewsCrawler å…§éƒ¨çš„å–®æœˆæ–‡ç« æŠ“å–é‚è¼¯
3. æ–°å¢ CLI åƒæ•¸ --max-concurrent-months / --max-concurrent-articles
4. è‡ªå‹•ä¿å­˜é€²åº¦ï¼Œå¯åœ¨ä¸­æ–·å¾ŒçºŒæ¥çºŒ
"""

import asyncio
import argparse
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict
from threading import Lock

# å°‡ç•¶å‰ç›®éŒ„åŠ å…¥ import è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

try:
    from crawl4ai_news_scraper import NewsCrawler
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° crawl4ai_news_scraper æ¨¡çµ„ï¼Œè«‹ç¢ºèªè©²æª”æ¡ˆèˆ‡æœ¬è…³æœ¬ä½æ–¼åŒä¸€è³‡æ–™å¤¾")
    sys.exit(1)

try:
    from utils import get_api_key, validate_date_format
except ImportError:
    # å¾Œå‚™å¯¦ä½œ (åƒ…ä¾›æ¸¬è©¦)
    def get_api_key():
        return os.getenv("FINNHUB_API_KEY", "YOUR_FINNHUB_API_KEY")

    def validate_date_format(date_str: str) -> bool:
        try:
            datetime.strptime(date_str, "%Y-%m-%d")
            return True
        except ValueError:
            return False


class BatchNewsDownloader:
    """æ‰¹é‡æ–°èä¸‹è¼‰å™¨ (æ”¯æ´è·¨æœˆä»½ä¸¦ç™¼)"""

    def __init__(
        self,
        api_key: str,
        output_base_dir: str = "batch_news_data",
        max_concurrent: int = 3,
        max_concurrent_articles: int = 5,
    ):
        self.api_key = api_key
        self.output_base_dir = Path(output_base_dir)
        self.output_base_dir.mkdir(exist_ok=True)

        # ä¸¦ç™¼æ§åˆ¶ (æœˆä»½å±¤ç´š)
        self.max_concurrent = max_concurrent
        self.month_semaphore = asyncio.Semaphore(max_concurrent)

        # æ–‡ç« å±¤ç´šä¸¦ç™¼ (å‚³å…¥ NewsCrawler)
        self.max_concurrent_articles = max_concurrent_articles

        # é€²åº¦èˆ‡æ—¥èªŒ
        self.progress_file = self.output_base_dir / "download_progress.json"
        self.log_file = self.output_base_dir / "download_log.txt"

        self.stats = {
            "total_months": 0,
            "successful_months": 0,
            "failed_months": 0,
            "skipped_months": 0,
            "total_articles": 0,
            "successful_articles": 0,
            "failed_articles": 0,
        }
        self.stats_lock = Lock()
        self._init_log()

    # ----------------------------------------
    # å…¬ç”¨å·¥å…·
    # ----------------------------------------
    def _init_log(self):
        with open(self.log_file, "w", encoding="utf-8") as f:
            f.write(f"æ‰¹é‡æ–°èä¸‹è¼‰é–‹å§‹æ™‚é–“: {datetime.now().isoformat()}\n")
            f.write("=" * 60 + "\n\n")

    def _log(self, msg: str, console: bool = True):
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(f"[{ts}] {msg}\n")
        if console:
            print(msg)

    def _load_progress(self) -> Dict:
        if self.progress_file.exists():
            try:
                return json.loads(self.progress_file.read_text(encoding="utf-8"))
            except Exception as e:
                self._log(f"è¼‰å…¥é€²åº¦å¤±æ•—: {e}")
        return {"completed_months": []}

    def _save_progress(self, completed_months: List[str]):
        data = {
            "completed_months": completed_months,
            "stats": self.stats,
            "last_update": datetime.now().isoformat(),
        }
        try:
            self.progress_file.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        except Exception as e:
            self._log(f"ä¿å­˜é€²åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def _update_stats(self, **delta):
        with self.stats_lock:
            for k, v in delta.items():
                if k in self.stats:
                    self.stats[k] += v

    # ----------------------------------------
    # æ ¸å¿ƒæµç¨‹
    # ----------------------------------------
    @staticmethod
    def _month_iter(start_dt: datetime, end_dt: datetime):
        cur_year, cur_month = start_dt.year, start_dt.month
        while (cur_year < end_dt.year) or (
            cur_year == end_dt.year and cur_month <= end_dt.month
        ):
            first_day = f"{cur_year}-{cur_month:02d}-01"
            # ä¸‹ä¸€å€‹æœˆ
            next_year = cur_year + (cur_month // 12)
            next_month = 1 if cur_month == 12 else cur_month + 1
            last_day_dt = datetime(next_year, next_month, 1) - timedelta(days=1)
            last_day = last_day_dt.strftime("%Y-%m-%d")
            yield cur_year, cur_month, first_day, last_day
            cur_year, cur_month = next_year, next_month

    async def _bounded_month_task(
        self,
        symbol: str,
        year: int,
        month: int,
        from_date: str,
        to_date: str,
        use_llm: bool,
        delay: float,
        max_articles: int,
        output_dir_name: str,
    ):
        async with self.month_semaphore:
            return await self._download_month_news(
                symbol,
                year,
                month,
                from_date,
                to_date,
                use_llm,
                delay,
                max_articles,
                output_dir_name,
            )

    async def _download_month_news(
        self,
        symbol: str,
        year: int,
        month: int,
        from_date: str,
        to_date: str,
        use_llm: bool,
        delay: float,
        max_articles: int,
        output_dir_name: str,
    ) -> Dict:
        month_key = f"{year}-{month:02d}"
        self._log(f"é–‹å§‹ä¸‹è¼‰ {symbol} {month_key} ({from_date} ~ {to_date})")

        month_dir = self.output_base_dir / output_dir_name
        month_dir.mkdir(parents=True, exist_ok=True)
        try:
            crawler = NewsCrawler(api_key=self.api_key, output_dir=str(month_dir))
            results = await crawler.crawl_company_news(
                symbol=symbol,
                from_date=from_date,
                to_date=to_date,
                use_llm=use_llm,
                delay=delay,
                max_articles=max_articles,
            )
            ok = sum(1 for r in results if r.get("success"))
            fail = len(results) - ok
            self._update_stats(
                total_articles=len(results),
                successful_articles=ok,
                failed_articles=fail,
            )
            self._log(f"âœ… {month_key} å®Œæˆ: {ok}/{len(results)} æˆåŠŸ")
            return {
                "month": month_key,
                "success": True,
                "total_articles": len(results),
                "successful_articles": ok,
                "failed_articles": fail,
                "output_dir": str(month_dir),
            }
        except Exception as e:
            self._log(f"âŒ {month_key} å¤±æ•—: {e}")
            return {
                "month": month_key,
                "success": False,
                "error": str(e),
                "total_articles": 0,
                "successful_articles": 0,
                "failed_articles": 0,
            }

    async def batch_download(
        self,
        symbol: str,
        start_date: str,
        end_date: str,
        use_llm: bool = False,
        delay: float = 0.5,
        max_articles_per_month: int | None = None,
        resume: bool = True,
    ) -> Dict:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
        months = list(self._month_iter(start_dt, end_dt))
        self.stats["total_months"] = len(months)

        self._log(f"å…± {len(months)} å€‹æœˆä»½ï¼Œå°‡ä½¿ç”¨ {self.max_concurrent} ä¸¦è¡Œæ•¸")
        completed_months = set()
        if resume:
            completed_months = set(self._load_progress().get("completed_months", []))
            if completed_months:
                self._log(f"æ¢å¾©ä¸‹è¼‰ï¼Œå·²å®Œæˆ {len(completed_months)} å€‹æœˆä»½")

        output_dir_name = f"{symbol.lower()}_news_{start_date.replace('-', '')}_{end_date.replace('-', '')}"

        # å»ºç«‹ Task æ¸…å–®
        tasks: list[asyncio.Task] = []
        for y, m, f, t in months:
            key = f"{y}-{m:02d}"
            if key in completed_months:
                self._log(f"è·³é {key} (å·²å®Œæˆ)")
                self._update_stats(skipped_months=1)
                continue
            coro = self._bounded_month_task(
                symbol,
                y,
                m,
                f,
                t,
                use_llm,
                delay,
                max_articles_per_month,
                output_dir_name,
            )
            tasks.append(asyncio.create_task(coro))

        results: List[Dict] = []
        idx = 0
        for task in asyncio.as_completed(tasks):
            idx += 1
            res = await task
            results.append(res)
            if res["success"]:
                self._update_stats(successful_months=1)
                completed_months.add(res["month"])
            else:
                self._update_stats(failed_months=1)
            self._save_progress(list(completed_months))
            success_rate = (
                self.stats["successful_months"]
                / max(1, self.stats["successful_months"] + self.stats["failed_months"])
                * 100
            )
            self._log(f"é€²åº¦ {idx}/{self.stats['total_months']}ï¼ŒæˆåŠŸç‡ {success_rate:.1f}%")

        return self._final_report(symbol, start_date, end_date, results)

    # ----------------------------------------
    # å ±å‘Š
    # ----------------------------------------
    def _final_report(self, symbol: str, start_date: str, end_date: str, results: List[Dict]):
        total_months_done = self.stats["successful_months"] + self.stats["failed_months"]
        month_rate = (
            self.stats["successful_months"] / max(1, total_months_done) * 100
        )
        article_rate = (
            self.stats["successful_articles"] / max(1, self.stats["total_articles"]) * 100
        )
        report = {
            "symbol": symbol,
            "date_range": f"{start_date}~{end_date}",
            "summary_time": datetime.now().isoformat(),
            "month_stats": {
                **{k: self.stats[k] for k in [
                    "total_months",
                    "successful_months",
                    "failed_months",
                    "skipped_months",
                ]},
                "success_rate": f"{month_rate:.2f}%",
            },
            "article_stats": {
                "total_articles": self.stats["total_articles"],
                "successful_articles": self.stats["successful_articles"],
                "failed_articles": self.stats["failed_articles"],
                "success_rate": f"{article_rate:.2f}%",
            },
            "output": str(self.output_base_dir),
            "details": results,
        }
        rpt_file = self.output_base_dir / f"report_{symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        rpt_file.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
        self._log("=" * 60)
        self._log(f"æ‰¹é‡ä¸‹è¼‰å®Œæˆï¼Œå ±å‘Šå­˜æ–¼ {rpt_file}")
        self._log("=" * 60)
        return report


# --------------------------------------------
# CLI å€æ®µ
# --------------------------------------------

def _parse_args():
    p = argparse.ArgumentParser(description="æ‰¹é‡æ–°èä¸‹è¼‰å™¨ - Crawl4AI")
    p.add_argument("--symbol", required=True, help="å…¬å¸è‚¡ç¥¨ä»£ç¢¼ï¼Œå¦‚ AAPL")
    p.add_argument("--from-date", required=True, help="é–‹å§‹æ—¥æœŸ YYYY-MM-DD")
    p.add_argument("--to-date", required=True, help="çµæŸæ—¥æœŸ YYYY-MM-DD")

    p.add_argument("--api-key", help="Finnhub API Keyï¼Œé è¨­è®€å–ç’°å¢ƒè®Šæ•¸ FINNHUB_API_KEY")
    p.add_argument("--output-dir", default="batch_news_data", help="è¼¸å‡ºè³‡æ–™å¤¾")
    p.add_argument("--use-llm", action="store_true", help="é¡å¤–ä½¿ç”¨ LLM èƒå–")
    p.add_argument("--delay", type=float, default=0.5, help="API å‘¼å«é–“éš”ç§’æ•¸")
    p.add_argument("--max-articles-per-month", type=int, help="å–®æœˆæœ€å¤šæ–‡ç« æ•¸")

    p.add_argument(
        "--max-concurrent-months", type=int, default=3, help="åŒæ™‚ä¸¦è¡Œä¸‹è¼‰æœˆä»½æ•¸")
    p.add_argument(
        "--max-concurrent-articles", type=int, default=5, help="å–®æœˆå…§ä¸¦è¡Œæ–‡ç« æ•¸")

    p.add_argument("--no-resume", action="store_true", help="å¿½ç•¥å…ˆå‰é€²åº¦")
    return p.parse_args()


async def _main():
    args = _parse_args()

    if not validate_date_format(args.from_date) or not validate_date_format(args.to_date):
        print("âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD")
        return

    start_dt = datetime.strptime(args.from_date, "%Y-%m-%d")
    end_dt = datetime.strptime(args.to_date, "%Y-%m-%d")
    if start_dt > end_dt:
        print("âŒ é–‹å§‹æ—¥æœŸä¸èƒ½æ™šæ–¼çµæŸæ—¥æœŸ")
        return

    api_key = args.api_key or get_api_key()
    if not api_key or api_key == "YOUR_FINNHUB_API_KEY":
        print("âŒ è«‹è¨­å®šæœ‰æ•ˆçš„ Finnhub API Key (FINNHUB_API_KEY)")
        return

    print("ğŸš€ é–‹å§‹æ‰¹é‡ä¸‹è¼‰ï¼š", args.symbol)
    downloader = BatchNewsDownloader(
        api_key=api_key,
        output_base_dir=args.output_dir,
        max_concurrent=args.max_concurrent_months,
        max_concurrent_articles=args.max_concurrent_articles,
    )

    try:
        await downloader.batch_download(
            symbol=args.symbol,
            start_date=args.from_date,
            end_date=args.to_date,
            use_llm=args.use_llm,
            delay=args.delay,
            max_articles_per_month=args.max_articles_per_month,
            resume=not args.no_resume,
        )
    except KeyboardInterrupt:
        print("â¹ï¸ ä¸­æ–·è¼¸å‡ºï¼Œå·²ä¿å­˜é€²åº¦ï¼Œå¯ä¸‹æ¬¡æ¢å¾©")


if __name__ == "__main__":
    asyncio.run(_main())