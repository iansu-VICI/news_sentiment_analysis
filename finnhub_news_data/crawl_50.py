from utils import get_company_news, finnhub_client, API_KEY
import argparse
from datetime import datetime, timedelta
import requests
import os
import re
from bs4 import BeautifulSoup
import time
from readability import Document
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError
import random
import json
from urllib.parse import urlparse
import urllib.parse

# --- Helper: Define a list of common User-Agents ---
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.7103.25 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
]

# --- Helper: Define common "load more" button texts/selectors ---
COMMON_LOAD_MORE_SELECTORS_TEXTS = [
    "[id*='show-more']",
    "[class*='load-more']",
    "[class*='show-more']",
    "[data-testid*='load-more']",
    "button:has-text('Show more')",
    "button:has-text('Load more')",
    "button:has-text('Read more')",
    "a:has-text('Continue reading')",
    "a.more-link"
]

def get_random_user_agent():
    """Selects a random User-Agent string."""
    return random.choice(USER_AGENTS)


def check_final_url_and_continue_reading(original_url, max_retries=3):
    """
    æª¢æŸ¥åŸå§‹URLçš„æœ€çµ‚URLï¼Œä¸¦ç¢ºèªæ˜¯å¦ç‚ºYahoo Financeé é¢ä¸”æœ‰Continue ReadingæŒ‰éˆ•
    
    Args:
        original_url: åŸå§‹æ–°èURL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
    Returns:
        dict: {
            'final_url': æœ€çµ‚URL,
            'is_yahoo_finance': æ˜¯å¦ç‚ºYahoo Financeé é¢,
            'has_continue_reading': æ˜¯å¦æœ‰Continue ReadingæŒ‰éˆ•,
            'status': 'success' or 'error',
            'error_message': éŒ¯èª¤è¨Šæ¯(å¦‚æœæœ‰)
        }
    """
    result = {
        'final_url': original_url,
        'is_yahoo_finance': False,
        'has_continue_reading': False,
        'status': 'error',
        'error_message': None
    }
    
    try:
        # å¦‚æœæ˜¯Finnhub API URLï¼Œå…ˆç²å–çœŸæ­£çš„æ–°èURL
        if "finnhub.io/api/news" in original_url:
            print(f"æª¢æ¸¬åˆ°Finnhub API URLï¼Œå˜—è©¦ç²å–çœŸæ­£çš„æ–°èURL...")
            
            try:
                # ä½¿ç”¨requestsç²å–APIéŸ¿æ‡‰
                headers = {
                    'User-Agent': get_random_user_agent(),
                    'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                }
                
                response = requests.get(original_url, headers=headers, timeout=10, allow_redirects=True)
                
                if response.status_code == 200:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯JSONéŸ¿æ‡‰
                    content_type = response.headers.get('content-type', '').lower()
                    if 'application/json' in content_type:
                        # å¦‚æœæ˜¯JSONï¼Œå¯èƒ½åŒ…å«çœŸæ­£çš„URL
                        try:
                            json_data = response.json()
                            # æŸ¥æ‰¾å¯èƒ½çš„URLå­—æ®µ
                            real_url = None
                            for key in ['url', 'link', 'href', 'source_url', 'original_url']:
                                if key in json_data and json_data[key]:
                                    real_url = json_data[key]
                                    break
                            
                            if real_url and real_url.startswith('http'):
                                print(f"å¾APIéŸ¿æ‡‰ä¸­ç²å–çœŸæ­£çš„æ–°èURL: {real_url}")
                                result['final_url'] = real_url
                            else:
                                print(f"APIéŸ¿æ‡‰ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„URL")
                                result['error_message'] = "APIéŸ¿æ‡‰ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„URL"
                                return result
                        except Exception as e:
                            print(f"è§£æAPI JSONéŸ¿æ‡‰æ™‚å‡ºéŒ¯: {e}")
                            result['error_message'] = f"è§£æAPI JSONéŸ¿æ‡‰æ™‚å‡ºéŒ¯: {e}"
                            return result
                    else:
                        # å¦‚æœä¸æ˜¯JSONï¼Œæª¢æŸ¥æ˜¯å¦æ˜¯HTMLé‡å®šå‘é é¢
                        if response.url != original_url:
                            print(f"API URLé‡å®šå‘åˆ°: {response.url}")
                            result['final_url'] = response.url
                        else:
                            # è§£æHTMLä¸­çš„é‡å®šå‘æˆ–éˆæ¥
                            soup = BeautifulSoup(response.text, 'html.parser')
                            
                            # æŸ¥æ‰¾metaé‡å®šå‘
                            meta_refresh = soup.find('meta', attrs={'http-equiv': 'refresh'})
                            if meta_refresh and 'content' in meta_refresh.attrs:
                                content = meta_refresh['content']
                                if 'url=' in content:
                                    redirect_url = content.split('url=')[1]
                                    print(f"å¾meta refreshç²å–é‡å®šå‘URL: {redirect_url}")
                                    result['final_url'] = redirect_url
                                    
                            # æŸ¥æ‰¾å¯èƒ½çš„éˆæ¥
                            if result['final_url'] == original_url:
                                links = soup.find_all('a', href=True)
                                for link in links:
                                    href = link['href']
                                    if href.startswith('http') and 'finnhub.io' not in href:
                                        print(f"å¾HTMLä¸­æ‰¾åˆ°å¤–éƒ¨éˆæ¥: {href}")
                                        result['final_url'] = href
                                        break
                else:
                    print(f"APIè«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                    result['error_message'] = f"APIè«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}"
                    return result
                    
            except Exception as e:
                print(f"è™•ç†Finnhub API URLæ™‚å‡ºéŒ¯: {e}")
                result['error_message'] = f"è™•ç†Finnhub API URLæ™‚å‡ºéŒ¯: {e}"
                return result
        
        # å¦‚æœç²å–åˆ°äº†çœŸæ­£çš„æ–°èURLï¼Œæˆ–è€…åŸæœ¬å°±ä¸æ˜¯API URLï¼Œç¹¼çºŒæª¢æŸ¥
        target_url = result['final_url']
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºYahoo Financeé é¢ï¼ˆå³ä½¿ä¸èƒ½è¨ªå•ä¹Ÿå¯ä»¥åˆ¤æ–·ï¼‰
        parsed_url = urlparse(target_url)
        result['is_yahoo_finance'] = 'finance.yahoo.com' in parsed_url.netloc
        
        # å¦‚æœä¸æ˜¯Yahoo Financeé é¢ï¼Œå¯ä»¥è·³éç€è¦½å™¨æª¢æŸ¥ï¼Œå› ç‚ºæˆ‘å€‘ä¸»è¦é—œå¿ƒYahoo Financeçš„Continue ReadingæŒ‰éˆ•
        if not result['is_yahoo_finance']:
            print(f"éYahoo Financeé é¢: {target_url}")
            result['status'] = 'success'
            result['has_continue_reading'] = False
            return result
        
        # åªæœ‰Yahoo Financeé é¢æ‰ä½¿ç”¨ç€è¦½å™¨æª¢æŸ¥Continue ReadingæŒ‰éˆ•
        print(f"æª¢æ¸¬åˆ°Yahoo Financeé é¢ï¼Œæª¢æŸ¥Continue ReadingæŒ‰éˆ•...")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent=get_random_user_agent(),
                viewport={'width': 1920, 'height': 1080}
            )
            page = context.new_page()
            
            for attempt in range(max_retries):
                try:
                    print(f"å˜—è©¦è¨ªå•Yahoo Finance URL: {target_url}")
                    
                    # å°èˆªåˆ°é é¢
                    response = page.goto(target_url, wait_until='domcontentloaded', timeout=15000)
                    
                    if response is None:
                        result['error_message'] = f"ç„¡æ³•å°èˆªåˆ°URL (å˜—è©¦ {attempt + 1}/{max_retries})"
                        continue
                    
                    # æ›´æ–°æœ€çµ‚URL
                    result['final_url'] = page.url
                    print(f"Yahoo Financeæœ€çµ‚URL: {result['final_url']}")
                    
                    # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
                    try:
                        page.wait_for_load_state('networkidle', timeout=8000)
                    except:
                        pass  # å¦‚æœç­‰å¾…è¶…æ™‚ä¹Ÿæ²’é—œä¿‚ï¼Œç¹¼çºŒæª¢æŸ¥
                    
                    # æª¢æŸ¥Continue ReadingæŒ‰éˆ•
                    continue_reading_selectors = [
                        "text='Continue Reading'",
                        "text='Read More'",
                        "[data-test*='continue']",
                        "[data-testid*='continue']",
                        "a:has-text('Continue')",
                        "button:has-text('Continue')",
                        ".continue-reading",
                        ".continue-reading-button",
                        ".story-continues",
                        "a[title='Continue Reading']",
                        "button[title='Continue Reading']",
                        "a[aria-label='Continue Reading']"
                    ]
                    
                    continue_button = None
                    for selector in continue_reading_selectors:
                        try:
                            continue_button = page.locator(selector).first
                            if continue_button.count() > 0:
                                print(f"æ‰¾åˆ°Continue ReadingæŒ‰éˆ•: {selector}")
                                break
                        except:
                            continue
                    
                    has_continue_reading = continue_button and continue_button.count() > 0
                    
                    # å¦‚æœæ‰¾åˆ°Continue ReadingæŒ‰éˆ•ï¼Œå˜—è©¦æå–href
                    if has_continue_reading:
                        try:
                            # æª¢æŸ¥æ˜¯å¦æ˜¯<a>æ¨™ç±¤ä¸¦æœ‰hrefå±¬æ€§
                            href = continue_button.get_attribute('href')
                            if href:
                                # å°‡ç›¸å°URLè½‰æ›ç‚ºçµ•å°URL
                                if href.startswith('/'):
                                    base_url = f"https://{urllib.parse.urlparse(result['final_url']).netloc}"
                                    href = urllib.parse.urljoin(base_url, href)
                                elif href.startswith('http'):
                                    # å·²ç¶“æ˜¯å®Œæ•´URL
                                    pass
                                else:
                                    # ç›¸å°è·¯å¾‘
                                    base_url = f"https://{urllib.parse.urlparse(result['final_url']).netloc}"
                                    href = urllib.parse.urljoin(base_url, href)
                                
                                print(f"Continue ReadingæŒ‰éˆ•æŒ‡å‘: {href}")
                                # æ›´æ–°æœ€çµ‚URLç‚ºContinue Readingçš„ç›®æ¨™
                                result['final_url'] = href
                                # é‡æ–°è¨ªå•çœŸæ­£çš„æ–‡ç« é é¢
                                try:
                                    page.goto(result['final_url'], wait_until='domcontentloaded', timeout=10000)
                                    print(f"å·²è·³è½‰åˆ°çœŸæ­£çš„æ–‡ç« é é¢: {result['final_url']}")
                                except Exception as e:
                                    print(f"è·³è½‰åˆ°æ–‡ç« é é¢å¤±æ•—: {e}")
                        except Exception as e:
                            print(f"æå–Continue Reading hrefå¤±æ•—: {e}")
                    
                    result['has_continue_reading'] = has_continue_reading
                    status = "ğŸ“° Yahoo Finance" if 'yahoo.com' in result['final_url'] else "ğŸŒ å…¶ä»–ç¶²ç«™"
                    continue_status = "ğŸ“– æœ‰Continue Reading" if has_continue_reading else "ğŸ“„ ç„¡Continue Reading"
                    
                    result['status'] = 'success'
                    break
                    
                except Exception as e:
                    result['error_message'] = f"å˜—è©¦ {attempt + 1}/{max_retries} å¤±æ•—: {str(e)}"
                    if attempt == max_retries - 1:
                        # å³ä½¿ç€è¦½å™¨æª¢æŸ¥å¤±æ•—ï¼Œå¦‚æœæˆ‘å€‘å·²ç¶“ç¢ºå®šäº†URLï¼Œä¹Ÿç®—éƒ¨åˆ†æˆåŠŸ
                        if result['final_url'] != original_url:
                            result['status'] = 'partial_success'
                            print(f"é›–ç„¶ç„¡æ³•å®Œå…¨è¨ªå•é é¢ï¼Œä½†å·²æˆåŠŸç²å–æœ€çµ‚URL")
                        break
                    time.sleep(1)  # é‡è©¦å‰ç­‰å¾…
            
            browser.close()
            
    except Exception as e:
        result['error_message'] = f"ç€è¦½å™¨æ“ä½œå¤±æ•—: {str(e)}"
        # å³ä½¿å‡ºç¾ç•°å¸¸ï¼Œå¦‚æœå·²ç¶“ç²å–äº†final_urlï¼Œä¹Ÿæ¨™è¨˜ç‚ºéƒ¨åˆ†æˆåŠŸ
        if result['final_url'] != original_url:
            result['status'] = 'partial_success'
    
    return result

def display_company_news(symbol, from_date=None, to_date=None, download_articles=False, output_dir="downloaded_articles", headless=True):
    """
    å–å¾—ä¸¦é¡¯ç¤ºç‰¹å®šå…¬å¸çš„æ–°èï¼Œåªä¿å­˜åŸºæœ¬è³‡è¨Šåˆ°JSONæ–‡ä»¶
    
    Args:
        symbol: å…¬å¸è‚¡ç¥¨ä»£ç¢¼
        from_date: èµ·å§‹æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
        to_date: çµæŸæ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
        download_articles: æ­¤åƒæ•¸åœ¨æ–°ç‰ˆæœ¬ä¸­è¢«å¿½ç•¥
        output_dir: JSONæ–‡ä»¶ä¿å­˜ç›®éŒ„
        headless: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼é‹è¡Œç€è¦½å™¨
    """
    # å¦‚æœæœªæŒ‡å®šæ—¥æœŸç¯„åœï¼Œé è¨­ç‚º2021-01-01åˆ°2025-05-28
    if from_date is None or to_date is None:
        from_date = from_date or '2021-01-01'
        to_date = to_date or '2025-05-28'
    
    print(f"\n--- æ­£åœ¨ç²å– {symbol} å¾ {from_date} åˆ° {to_date} çš„å…¬å¸æ–°è ---")
    company_news = get_company_news(symbol, from_date, to_date)
    
    if company_news:
        print(f"å…±ç²å– {len(company_news)} æ¢æ–°èã€‚")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        os.makedirs(output_dir, exist_ok=True)
        
        # æº–å‚™æ–°èè³‡æ–™åˆ—è¡¨
        news_data = []
        
        print(f"\né–‹å§‹è™•ç†æ–°èè³‡æ–™...")
        processed_count = 0
        filtered_count = 0  # è¢«éæ¿¾æ‰çš„æ•¸é‡
        
        for i, news_item in enumerate(company_news):
            original_url = news_item.get('url')
            if not original_url:
                continue
                
            print(f"\nè™•ç†æ–°è {i+1}/{len(company_news)}: {original_url[:80]}...")
            
            # æª¢æŸ¥æœ€çµ‚URLå’ŒContinue ReadingæŒ‰éˆ•
            url_check_result = check_final_url_and_continue_reading(original_url)
            
            # æª¢æŸ¥æ˜¯å¦æˆåŠŸè§£æå‡ºä¸åŒçš„final_url
            if url_check_result['final_url'] == original_url:
                filtered_count += 1
                print(f"  âš ï¸  è·³éï¼šoriginal_urlèˆ‡final_urlç›¸åŒï¼Œå¯èƒ½è§£æå¤±æ•—")
                continue
            
            # æº–å‚™æ–°èè³‡æ–™
            news_info = {
                'headline': news_item.get('headline'),
                'source': news_item.get('source'),
                'datetime': news_item.get('datetime'),
                'publish_date': datetime.fromtimestamp(news_item.get('datetime')).strftime('%Y-%m-%d %H:%M:%S'),
                'original_url': original_url,
                'final_url': url_check_result['final_url'],
                'is_yahoo_finance': url_check_result['is_yahoo_finance'],
                'has_continue_reading': url_check_result['has_continue_reading'],
                'url_check_status': url_check_result['status'],
                'url_check_error': url_check_result.get('error_message')
            }
            
            news_data.append(news_info)
            processed_count += 1
            
            # é¡¯ç¤ºè™•ç†çµæœ
            status_msg = "âœ…" if url_check_result['status'] == 'success' else "ğŸ”„" if url_check_result['status'] == 'partial_success' else "âŒ"
            yahoo_msg = "ğŸ“° Yahoo Finance" if url_check_result['is_yahoo_finance'] else "ğŸŒ å…¶ä»–ç¶²ç«™"
            continue_msg = "ğŸ“– æœ‰Continue Reading" if url_check_result['has_continue_reading'] else "ğŸ“„ ç„¡Continue Reading"
            
            print(f"  {status_msg} {yahoo_msg}, {continue_msg}")
            
            # é¿å…è«‹æ±‚éæ–¼é »ç¹
            time.sleep(0.5)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        output_file = os.path.join(output_dir, f"{symbol.lower()}.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'symbol': symbol,
                'from_date': from_date,
                'to_date': to_date,
                'total_news_fetched': len(company_news),
                'filtered_out': filtered_count,
                'valid_news': len(news_data),
                'processed_count': processed_count,
                'generated_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'news_data': news_data
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\næ–°èè³‡æ–™è™•ç†å®Œæˆã€‚")
        print(f"  ç¸½ç²å–æ–°è: {len(company_news)}")
        print(f"  æˆåŠŸè§£æ: {processed_count}")
        print(f"  éæ¿¾æ‰: {filtered_count} (original_urlèˆ‡final_urlç›¸åŒ)")
        print(f"è³‡æ–™å·²ä¿å­˜åˆ°: {output_file}")
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        yahoo_finance_count = sum(1 for news in news_data if news['is_yahoo_finance'])
        continue_reading_count = sum(1 for news in news_data if news['has_continue_reading'])
        
        print(f"\nçµ±è¨ˆè³‡è¨Š:")
        print(f"  æœ‰æ•ˆæ–°èæ•¸: {len(news_data)}")
        print(f"  Yahoo Finance æ–°è: {yahoo_finance_count}")
        print(f"  æœ‰ Continue Reading æŒ‰éˆ•: {continue_reading_count}")
        
    else:
        print("æœªèƒ½ç²å–å…¬å¸æ–°èï¼Œæˆ–è©²æ™‚æ®µå…§ç„¡æ–°èã€‚")
    
    return company_news

def get_market_news(category="general", min_id=0):
    """
    ç²å–ä¸€èˆ¬å¸‚å ´æ–°èã€‚

    :param category: æ–°èé¡åˆ¥ (ä¾‹å¦‚ general, forex, crypto, merger)
    :param min_id: (ç”¨æ–¼åˆ†é ) åªç²å– ID å¤§æ–¼ min_id çš„æ–°è
    :return: æ–°èåˆ—è¡¨ (JSON)
    """
    try:
        # ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±ï¼Œå³ general_news è€Œé market_news
        if category in ["general", "forex", "crypto", "merger"]:
            return finnhub_client.general_news(category=category)
        elif category == "crypto":
            # ç²å–åŠ å¯†è²¨å¹£ç‰¹å®šæ–°è
            # æ³¨æ„ï¼šå¦‚æœ general_news å·²æ”¯æ´ cryptoï¼Œé€™éƒ¨åˆ†å¯èƒ½é‡è¤‡
            crypto_news = []
            
            # å˜—è©¦ç²å–åŠ å¯†è²¨å¹£ç‰¹å®šçš„æ–°èï¼ˆå¦‚æœæœ‰å°ˆç”¨APIï¼‰
            try:
                crypto_news = finnhub_client.crypto_news()
            except:
                # å¦‚æœæ²’æœ‰å°ˆç”¨APIï¼Œå›é€€åˆ°ä½¿ç”¨ general_news çš„ crypto é¡åˆ¥
                crypto_news = finnhub_client.general_news(category="crypto")
                
            return crypto_news
        else:
            return finnhub_client.general_news(category="general")
    except Exception as e:
        print(f"ç²å–å¸‚å ´æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def display_market_news(category="general", min_id=0):
    """é¡¯ç¤ºå¸‚å ´æ–°è"""
    print(f"\n--- æ­£åœ¨ç²å–{category}å¸‚å ´æ–°è ---")
    market_news_data = get_market_news(category=category, min_id=min_id)
    
    if market_news_data:
        print(f"å…±ç²å– {len(market_news_data)} æ¢å¸‚å ´æ–°èã€‚")
        for i, news_item in enumerate(market_news_data[:3]): # åªé¡¯ç¤ºå‰3æ¢
            news_datetime = datetime.fromtimestamp(news_item.get('datetime')).strftime('%Y-%m-%d %H:%M:%S')
            print(f"\nå¸‚å ´æ–°è {i+1}:")
            print(f"  æ¨™é¡Œ: {news_item.get('headline')}")
            print(f"  ä¾†æº: {news_item.get('source')}")
            print(f"  ç™¼å¸ƒæ™‚é–“: {news_datetime}")
    else:
        print("æœªèƒ½ç²å–å¸‚å ´æ–°èã€‚")
    
    return market_news_data

def parse_args():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(description='ç²å–é‡‘èæ–°è')
    parser.add_argument('--type', type=str, choices=['company', 'market'], default='company',
                      help='æ–°èé¡å‹: company (å…¬å¸æ–°è) æˆ– market (å¸‚å ´æ–°è)')
    parser.add_argument('--symbol', type=str, default='SOFI',
                      help='å…¬å¸è‚¡ç¥¨ä»£ç¢¼ (é è¨­: AAPL)')
    parser.add_argument('--from-date', type=str, default='2021-01-01',
                      help='èµ·å§‹æ—¥æœŸ (YYYY-MM-DDæ ¼å¼ï¼Œé è¨­: 2025-0-01)')
    parser.add_argument('--to-date', type=str, default='2021-01-30',
                      help='çµæŸæ—¥æœŸ (YYYY-MM-DDæ ¼å¼ï¼Œé è¨­: 2025-04-01)')
    parser.add_argument('--category', type=str, default='general',
                      choices=['general', 'forex', 'crypto', 'merger'],
                      help='å¸‚å ´æ–°èé¡åˆ¥ (é è¨­: general)')
    parser.add_argument('--min-id', type=int, default=0,
                      help='å¸‚å ´æ–°èæœ€å°ID (ç”¨æ–¼åˆ†é ï¼Œé è¨­: 0)')
    parser.add_argument('--download-articles', action='store_true',
                      help='ä¸‹è¼‰æ–‡ç« å…§å®¹ (é è¨­: False)')
    parser.add_argument('--output-dir', type=str, default='downloaded_articles',
                      help='æ–‡ç« ä¿å­˜ç›®éŒ„ (é è¨­: downloaded_articles)')
    parser.add_argument('--no-headless', action='store_true',
                      help='ä½¿ç”¨æœ‰é ­æ¨¡å¼é‹è¡Œç€è¦½å™¨ (é è¨­: ç„¡é ­æ¨¡å¼)')
    
    args = parser.parse_args()
    
    # è½‰æ›no-headlessåˆ°headless
    args.headless = not args.no_headless
    
    # æª¢æŸ¥X serveræ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨ä½†è¦æ±‚æœ‰é ­æ¨¡å¼å‰‡é¡¯ç¤ºè­¦å‘Š
    if args.no_headless and not os.environ.get('DISPLAY'):
        print("\nè­¦å‘Š: ä½ è«‹æ±‚äº†æœ‰é ­æ¨¡å¼ (--no-headless)ï¼Œä½†æ²’æœ‰æª¢æ¸¬åˆ°X serverã€‚")
        print("ä½ æœ‰å…©å€‹é¸æ“‡:")
        print("  1. ç§»é™¤ --no-headless åƒæ•¸ï¼Œä½¿ç”¨ç„¡é ­æ¨¡å¼é‹è¡Œ")
        print("  2. å®‰è£ä¸¦ä½¿ç”¨xvfb: sudo apt-get install xvfb")
        print("     ç„¶å¾Œé‹è¡Œ: xvfb-run python crawl_50.py [å…¶ä»–åƒæ•¸] --no-headless")
        print("\nè‡ªå‹•åˆ‡æ›åˆ°ç„¡é ­æ¨¡å¼ç¹¼çºŒåŸ·è¡Œ...\n")
        args.headless = True
    
    return args

# --- ä½¿ç”¨ç¯„ä¾‹ ---
if __name__ == "__main__":
    if API_KEY == "YOUR_FINNHUB_API_KEY":
        print("è«‹å…ˆåœ¨ç¨‹å¼ç¢¼ä¸­è¨­å®šæ‚¨çš„ API_KEY")
    else:
        # è§£æå‘½ä»¤åˆ—åƒæ•¸
        args = parse_args()
        
        # æ ¹æ“šæ–°èé¡å‹èª¿ç”¨ç›¸æ‡‰çš„å‡½å¼
        if args.type == 'company':
            display_company_news(args.symbol, args.from_date, args.to_date, 
                                args.download_articles, args.output_dir, args.headless)
        else:  # market news
            display_market_news(args.category, args.min_id)